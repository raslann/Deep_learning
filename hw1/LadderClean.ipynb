{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pickle \n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from sub import subMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainset_labeled = pickle.load(open(\"train_labeled.p\", \"rb\")) \n",
    "train_loader  = torch.utils.data.DataLoader(trainset_labeled, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "validset = pickle.load(open(\"validation.p\", \"rb\"))\n",
    "valid_loader = torch.utils.data.DataLoader(validset, batch_size=64, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#utility functions\n",
    "def rand(x, level = 1e-3):\n",
    "    return x + torch.randn(x.size()) * level\n",
    "\n",
    "def norm_weights_2d(size):\n",
    "    return BatchNorm2d(nn.Linear(size))\n",
    "def norm_weights_1d(size):\n",
    "    return BatchNorm2d(nn.Linear(size))\n",
    "\n",
    "def reluN(x, level = 1e-3):\n",
    "    y = x + torch.randn(x.size()) * level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net (\n",
      ")\n",
      "Models has 0 learnable paramater:\n"
     ]
    }
   ],
   "source": [
    "def norm(size, dim = 1):\n",
    "    if dim == 1:\n",
    "        return torch.nn.BatchNorm1d(size)\n",
    "    else:\n",
    "        return torch.nn.BatchNorm2d(size)\n",
    "def linear(indim, outdim):\n",
    "    return nn.Linear(indim, outdim)\n",
    "def var(tens):\n",
    "    return torch.autograd.Variable(tens)\n",
    "def randn(size):\n",
    "    return torch.randn(size)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers = [1000, 600, 300, 10]\n",
    "        layers = self.layers\n",
    "        self.encs = [28*28, layers[0]]\n",
    "        self.lats = []\n",
    "        self.encnorms = []\n",
    "        self.decs = []\n",
    "        self.decnorms = []\n",
    "        for idx, l in enumerate(layers[:-1]):\n",
    "            self.lats.append(linear(l, l))\n",
    "            self.encnorms.append(norm(l))\n",
    "            self.encs.append(linear(l, layers[idx+1]))\n",
    "            self.decs.append(linear(layers[idx+1], l))\n",
    "            self.decnorms.append(norm(l))\n",
    "        self.batch_size = 64\n",
    "        self.weights = [self.encs, self.lats, self.decs]\n",
    "        \n",
    "\n",
    "    def forward(self, x, v = 0, noise=1e-3):\n",
    "        self.eps = noise\n",
    "        self.batch_size = x.size()[0]\n",
    "        bs = self.batch_size\n",
    "        #enc= F.relu(self.enc4(F.relu(self.enc3(F.relu(self.enc2(F.relu(self.enc1(x))))))))\n",
    "        corrupted = []\n",
    "        corruptedout = []\n",
    "        for idx, l in enumerate(self.layers):\n",
    "            if idx == 0:\n",
    "                corrupted.append(self.encnorms[l](self.encs[l](x)))\n",
    "            else:\n",
    "                corrupted.append(self.encnorms[l](self.encs[l](corruptedout[-1])))\n",
    "            corruptedout.append(F.relu(self.encs[l] + self.encnorms[l].weight.unsqueeze(0).expand(\n",
    "                    bs, l) * var(randn(corrupted[-1].size()) * self.eps) +\n",
    "                    self.encnorms[0].bias.unsqueeze(0).expand(bs, l)))\n",
    "            \n",
    "        encout = F.softmax(corruptedout[-1])\n",
    "        \n",
    "        clean = [x]\n",
    "        for norm, enc in zip(self.encnorms, self.encs):\n",
    "            clean.append(F.relu(norm(enc(clean[-1]))))\n",
    "        \n",
    "        decout = [encout]\n",
    "        decin = []\n",
    "        for idx in range(len(self.layers) - 1, -1, -1):\n",
    "            dec = self.decs[idx]\n",
    "            lat = self.lats[idx]\n",
    "            decnorm = self.decnorms[idx+1]\n",
    "            decin.append(dec(decout[-1]) + lat(corruptedout[idx]))\n",
    "            decout.append(F.relu(decin[-1] + decnorm.weight.unsqueeze(0).expand(bs, self.layers[idx]) *\n",
    "                    var(randn(decin[-1].size()) * self.eps) + \\\n",
    "                    decnorm.bias.unsqueeze(0).expend(bs, self.layers[idx])))\n",
    "        weight_reg = 0\n",
    "        for w_list in self.weights:\n",
    "            for w in w_list:\n",
    "                weight_reg += (w.weight**2).mean()/100\n",
    "        \n",
    "        yhat = F.log_softmax(corruptedout[-1])\n",
    "        \n",
    "        encode_err = 0\n",
    "        enc_weight = 5\n",
    "        enc_decay = .2\n",
    "        for c, d in zip(clean[-2::-1], decin):\n",
    "            encode_err += enc_weight * ((c - d)**2).mean()\n",
    "        return yhat, weight_reg + encode_err\n",
    "        \n",
    "\n",
    "model = Net()\n",
    "params = list(model.parameters())\n",
    "\n",
    "if 1:\n",
    "    print(model)\n",
    "    print('Models has {} learnable paramater:'.format(len(params)))\n",
    "    [print('parameter {} has a size of {}'.format(i+1, params[i].size())) for i in range(len(params))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ba8063e90186>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/lee/anaconda3/lib/python3.5/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, momentum, dampening, weight_decay)\u001b[0m\n\u001b[1;32m     24\u001b[0m         defaults = dict(lr=lr, momentum=momentum, dampening=dampening,\n\u001b[1;32m     25\u001b[0m                         weight_decay=weight_decay)\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lee/anaconda3/lib/python3.5/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target) # Wrap them in Variable \n",
    "        optimizer.zero_grad() # Zero the parameter gradients\n",
    "        data = data.view(data.size()[0], 28*28)\n",
    "        outputs = model(data,noise=1e-2) # Forward \n",
    "        output = outputs[0]\n",
    "        US = outputs[1]\n",
    "        loss = F.nll_loss(output, target) + US*.1\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(epoch, valid_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in valid_loader:\n",
    "\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        data = data.view(data.size()[0], 28*28)\n",
    "        outputs = model(data, noise = 0)\n",
    "        output = outputs[0]\n",
    "        test_loss += F.nll_loss(output, target).data[0]\n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    test_loss /= len(valid_loader) # loss function already averages over batch size\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(valid_loader.dataset),\n",
    "        100. * correct / len(valid_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/3000 (0%)]\tLoss: 2.397224\n",
      "Train Epoch: 1 [640/3000 (21%)]\tLoss: 1.519292\n",
      "Train Epoch: 1 [1280/3000 (43%)]\tLoss: 1.349676\n",
      "Train Epoch: 1 [1920/3000 (64%)]\tLoss: 1.206272\n",
      "Train Epoch: 1 [2560/3000 (85%)]\tLoss: 1.036737\n",
      "\n",
      "Test set: Average loss: 0.7336, Accuracy: 8942/10000 (89%)\n",
      "\n",
      "Train Epoch: 2 [0/3000 (0%)]\tLoss: 0.859665\n",
      "Train Epoch: 2 [640/3000 (21%)]\tLoss: 0.651262\n",
      "Train Epoch: 2 [1280/3000 (43%)]\tLoss: 0.693339\n",
      "Train Epoch: 2 [1920/3000 (64%)]\tLoss: 0.505255\n",
      "Train Epoch: 2 [2560/3000 (85%)]\tLoss: 0.446675\n",
      "\n",
      "Test set: Average loss: 0.4306, Accuracy: 9278/10000 (93%)\n",
      "\n",
      "Train Epoch: 3 [0/3000 (0%)]\tLoss: 0.394293\n",
      "Train Epoch: 3 [640/3000 (21%)]\tLoss: 0.412965\n",
      "Train Epoch: 3 [1280/3000 (43%)]\tLoss: 0.304297\n",
      "Train Epoch: 3 [1920/3000 (64%)]\tLoss: 0.339598\n",
      "Train Epoch: 3 [2560/3000 (85%)]\tLoss: 0.257155\n",
      "\n",
      "Test set: Average loss: 0.3471, Accuracy: 9367/10000 (94%)\n",
      "\n",
      "Train Epoch: 4 [0/3000 (0%)]\tLoss: 0.294940\n",
      "Train Epoch: 4 [640/3000 (21%)]\tLoss: 0.221244\n",
      "Train Epoch: 4 [1280/3000 (43%)]\tLoss: 0.227718\n",
      "Train Epoch: 4 [1920/3000 (64%)]\tLoss: 0.191122\n",
      "Train Epoch: 4 [2560/3000 (85%)]\tLoss: 0.235295\n",
      "\n",
      "Test set: Average loss: 0.2937, Accuracy: 9388/10000 (94%)\n",
      "\n",
      "Train Epoch: 5 [0/3000 (0%)]\tLoss: 0.201335\n",
      "Train Epoch: 5 [640/3000 (21%)]\tLoss: 0.168827\n",
      "Train Epoch: 5 [1280/3000 (43%)]\tLoss: 0.176675\n",
      "Train Epoch: 5 [1920/3000 (64%)]\tLoss: 0.155908\n",
      "Train Epoch: 5 [2560/3000 (85%)]\tLoss: 0.146970\n",
      "\n",
      "Test set: Average loss: 0.2662, Accuracy: 9413/10000 (94%)\n",
      "\n",
      "Train Epoch: 6 [0/3000 (0%)]\tLoss: 0.141617\n",
      "Train Epoch: 6 [640/3000 (21%)]\tLoss: 0.171516\n",
      "Train Epoch: 6 [1280/3000 (43%)]\tLoss: 0.115817\n",
      "Train Epoch: 6 [1920/3000 (64%)]\tLoss: 0.185126\n",
      "Train Epoch: 6 [2560/3000 (85%)]\tLoss: 0.120702\n",
      "\n",
      "Test set: Average loss: 0.2484, Accuracy: 9444/10000 (94%)\n",
      "\n",
      "Train Epoch: 7 [0/3000 (0%)]\tLoss: 0.158536\n",
      "Train Epoch: 7 [640/3000 (21%)]\tLoss: 0.103796\n",
      "Train Epoch: 7 [1280/3000 (43%)]\tLoss: 0.117394\n",
      "Train Epoch: 7 [1920/3000 (64%)]\tLoss: 0.124200\n",
      "Train Epoch: 7 [2560/3000 (85%)]\tLoss: 0.094162\n",
      "\n",
      "Test set: Average loss: 0.2410, Accuracy: 9438/10000 (94%)\n",
      "\n",
      "Train Epoch: 8 [0/3000 (0%)]\tLoss: 0.097506\n",
      "Train Epoch: 8 [640/3000 (21%)]\tLoss: 0.118158\n",
      "Train Epoch: 8 [1280/3000 (43%)]\tLoss: 0.126496\n",
      "Train Epoch: 8 [1920/3000 (64%)]\tLoss: 0.089644\n",
      "Train Epoch: 8 [2560/3000 (85%)]\tLoss: 0.098320\n",
      "\n",
      "Test set: Average loss: 0.2323, Accuracy: 9457/10000 (95%)\n",
      "\n",
      "Train Epoch: 9 [0/3000 (0%)]\tLoss: 0.098244\n",
      "Train Epoch: 9 [640/3000 (21%)]\tLoss: 0.086710\n",
      "Train Epoch: 9 [1280/3000 (43%)]\tLoss: 0.085278\n",
      "Train Epoch: 9 [1920/3000 (64%)]\tLoss: 0.096873\n",
      "Train Epoch: 9 [2560/3000 (85%)]\tLoss: 0.095363\n",
      "\n",
      "Test set: Average loss: 0.2325, Accuracy: 9420/10000 (94%)\n",
      "\n",
      "Train Epoch: 10 [0/3000 (0%)]\tLoss: 0.081407\n",
      "Train Epoch: 10 [640/3000 (21%)]\tLoss: 0.109604\n",
      "Train Epoch: 10 [1280/3000 (43%)]\tLoss: 0.087081\n",
      "Train Epoch: 10 [1920/3000 (64%)]\tLoss: 0.092243\n",
      "Train Epoch: 10 [2560/3000 (85%)]\tLoss: 0.100741\n",
      "\n",
      "Test set: Average loss: 0.2203, Accuracy: 9438/10000 (94%)\n",
      "\n",
      "Train Epoch: 11 [0/3000 (0%)]\tLoss: 0.076525\n",
      "Train Epoch: 11 [640/3000 (21%)]\tLoss: 0.101939\n",
      "Train Epoch: 11 [1280/3000 (43%)]\tLoss: 0.102537\n",
      "Train Epoch: 11 [1920/3000 (64%)]\tLoss: 0.084631\n",
      "Train Epoch: 11 [2560/3000 (85%)]\tLoss: 0.111299\n",
      "\n",
      "Test set: Average loss: 0.2252, Accuracy: 9423/10000 (94%)\n",
      "\n",
      "Train Epoch: 12 [0/3000 (0%)]\tLoss: 0.077037\n",
      "Train Epoch: 12 [640/3000 (21%)]\tLoss: 0.072022\n",
      "Train Epoch: 12 [1280/3000 (43%)]\tLoss: 0.072668\n",
      "Train Epoch: 12 [1920/3000 (64%)]\tLoss: 0.090856\n",
      "Train Epoch: 12 [2560/3000 (85%)]\tLoss: 0.078507\n",
      "\n",
      "Test set: Average loss: 0.2155, Accuracy: 9440/10000 (94%)\n",
      "\n",
      "Train Epoch: 13 [0/3000 (0%)]\tLoss: 0.069558\n",
      "Train Epoch: 13 [640/3000 (21%)]\tLoss: 0.079949\n",
      "Train Epoch: 13 [1280/3000 (43%)]\tLoss: 0.069372\n",
      "Train Epoch: 13 [1920/3000 (64%)]\tLoss: 0.091629\n",
      "Train Epoch: 13 [2560/3000 (85%)]\tLoss: 0.126736\n",
      "\n",
      "Test set: Average loss: 0.2329, Accuracy: 9377/10000 (94%)\n",
      "\n",
      "Train Epoch: 14 [0/3000 (0%)]\tLoss: 0.084599\n",
      "Train Epoch: 14 [640/3000 (21%)]\tLoss: 0.071580\n",
      "Train Epoch: 14 [1280/3000 (43%)]\tLoss: 0.099055\n",
      "Train Epoch: 14 [1920/3000 (64%)]\tLoss: 0.076243\n",
      "Train Epoch: 14 [2560/3000 (85%)]\tLoss: 0.080496\n",
      "\n",
      "Test set: Average loss: 0.2206, Accuracy: 9419/10000 (94%)\n",
      "\n",
      "Train Epoch: 15 [0/3000 (0%)]\tLoss: 0.073408\n",
      "Train Epoch: 15 [640/3000 (21%)]\tLoss: 0.065708\n",
      "Train Epoch: 15 [1280/3000 (43%)]\tLoss: 0.060832\n",
      "Train Epoch: 15 [1920/3000 (64%)]\tLoss: 0.230287\n",
      "Train Epoch: 15 [2560/3000 (85%)]\tLoss: 0.129932\n",
      "\n",
      "Test set: Average loss: 0.2237, Accuracy: 9393/10000 (94%)\n",
      "\n",
      "Train Epoch: 16 [0/3000 (0%)]\tLoss: 0.061855\n",
      "Train Epoch: 16 [640/3000 (21%)]\tLoss: 0.065582\n",
      "Train Epoch: 16 [1280/3000 (43%)]\tLoss: 0.060542\n",
      "Train Epoch: 16 [1920/3000 (64%)]\tLoss: 0.075958\n",
      "Train Epoch: 16 [2560/3000 (85%)]\tLoss: 0.065657\n",
      "\n",
      "Test set: Average loss: 0.2109, Accuracy: 9431/10000 (94%)\n",
      "\n",
      "Train Epoch: 17 [0/3000 (0%)]\tLoss: 0.061675\n",
      "Train Epoch: 17 [640/3000 (21%)]\tLoss: 0.060979\n",
      "Train Epoch: 17 [1280/3000 (43%)]\tLoss: 0.065527\n",
      "Train Epoch: 17 [1920/3000 (64%)]\tLoss: 0.056663\n",
      "Train Epoch: 17 [2560/3000 (85%)]\tLoss: 0.057665\n",
      "\n",
      "Test set: Average loss: 0.2042, Accuracy: 9468/10000 (95%)\n",
      "\n",
      "Train Epoch: 18 [0/3000 (0%)]\tLoss: 0.064260\n",
      "Train Epoch: 18 [640/3000 (21%)]\tLoss: 0.058882\n",
      "Train Epoch: 18 [1280/3000 (43%)]\tLoss: 0.051842\n",
      "Train Epoch: 18 [1920/3000 (64%)]\tLoss: 0.071087\n",
      "Train Epoch: 18 [2560/3000 (85%)]\tLoss: 0.055273\n",
      "\n",
      "Test set: Average loss: 0.2004, Accuracy: 9467/10000 (95%)\n",
      "\n",
      "Train Epoch: 19 [0/3000 (0%)]\tLoss: 0.057432\n",
      "Train Epoch: 19 [640/3000 (21%)]\tLoss: 0.051962\n",
      "Train Epoch: 19 [1280/3000 (43%)]\tLoss: 0.052465\n",
      "Train Epoch: 19 [1920/3000 (64%)]\tLoss: 0.054326\n",
      "Train Epoch: 19 [2560/3000 (85%)]\tLoss: 0.057703\n",
      "\n",
      "Test set: Average loss: 0.2038, Accuracy: 9450/10000 (94%)\n",
      "\n",
      "Train Epoch: 20 [0/3000 (0%)]\tLoss: 0.053699\n",
      "Train Epoch: 20 [640/3000 (21%)]\tLoss: 0.055212\n",
      "Train Epoch: 20 [1280/3000 (43%)]\tLoss: 0.082111\n",
      "Train Epoch: 20 [1920/3000 (64%)]\tLoss: 0.056425\n",
      "Train Epoch: 20 [2560/3000 (85%)]\tLoss: 0.077744\n",
      "\n",
      "Test set: Average loss: 0.2126, Accuracy: 9431/10000 (94%)\n",
      "\n",
      "Train Epoch: 21 [0/3000 (0%)]\tLoss: 0.057251\n",
      "Train Epoch: 21 [640/3000 (21%)]\tLoss: 0.096231\n",
      "Train Epoch: 21 [1280/3000 (43%)]\tLoss: 0.051422\n",
      "Train Epoch: 21 [1920/3000 (64%)]\tLoss: 0.058098\n",
      "Train Epoch: 21 [2560/3000 (85%)]\tLoss: 0.063015\n",
      "\n",
      "Test set: Average loss: 0.2267, Accuracy: 9382/10000 (94%)\n",
      "\n",
      "Train Epoch: 22 [0/3000 (0%)]\tLoss: 0.051948\n",
      "Train Epoch: 22 [640/3000 (21%)]\tLoss: 0.055223\n",
      "Train Epoch: 22 [1280/3000 (43%)]\tLoss: 0.051617\n",
      "Train Epoch: 22 [1920/3000 (64%)]\tLoss: 0.049840\n",
      "Train Epoch: 22 [2560/3000 (85%)]\tLoss: 0.050712\n",
      "\n",
      "Test set: Average loss: 0.2018, Accuracy: 9461/10000 (95%)\n",
      "\n",
      "Train Epoch: 23 [0/3000 (0%)]\tLoss: 0.049231\n",
      "Train Epoch: 23 [640/3000 (21%)]\tLoss: 0.050730\n",
      "Train Epoch: 23 [1280/3000 (43%)]\tLoss: 0.078762\n",
      "Train Epoch: 23 [1920/3000 (64%)]\tLoss: 0.052118\n",
      "Train Epoch: 23 [2560/3000 (85%)]\tLoss: 0.051402\n",
      "\n",
      "Test set: Average loss: 0.1988, Accuracy: 9452/10000 (95%)\n",
      "\n",
      "Train Epoch: 24 [0/3000 (0%)]\tLoss: 0.055395\n",
      "Train Epoch: 24 [640/3000 (21%)]\tLoss: 0.055295\n",
      "Train Epoch: 24 [1280/3000 (43%)]\tLoss: 0.053788\n",
      "Train Epoch: 24 [1920/3000 (64%)]\tLoss: 0.047333\n",
      "Train Epoch: 24 [2560/3000 (85%)]\tLoss: 0.043674\n",
      "\n",
      "Test set: Average loss: 0.2055, Accuracy: 9439/10000 (94%)\n",
      "\n",
      "Train Epoch: 25 [0/3000 (0%)]\tLoss: 0.043832\n",
      "Train Epoch: 25 [640/3000 (21%)]\tLoss: 0.043103\n",
      "Train Epoch: 25 [1280/3000 (43%)]\tLoss: 0.052655\n",
      "Train Epoch: 25 [1920/3000 (64%)]\tLoss: 0.043512\n",
      "Train Epoch: 25 [2560/3000 (85%)]\tLoss: 0.047888\n",
      "\n",
      "Test set: Average loss: 0.1969, Accuracy: 9461/10000 (95%)\n",
      "\n",
      "Train Epoch: 26 [0/3000 (0%)]\tLoss: 0.044513\n",
      "Train Epoch: 26 [640/3000 (21%)]\tLoss: 0.071785\n",
      "Train Epoch: 26 [1280/3000 (43%)]\tLoss: 0.066316\n",
      "Train Epoch: 26 [1920/3000 (64%)]\tLoss: 0.064337\n",
      "Train Epoch: 26 [2560/3000 (85%)]\tLoss: 0.050130\n",
      "\n",
      "Test set: Average loss: 0.2136, Accuracy: 9404/10000 (94%)\n",
      "\n",
      "Train Epoch: 27 [0/3000 (0%)]\tLoss: 0.054094\n",
      "Train Epoch: 27 [640/3000 (21%)]\tLoss: 0.050754\n",
      "Train Epoch: 27 [1280/3000 (43%)]\tLoss: 0.043573\n",
      "Train Epoch: 27 [1920/3000 (64%)]\tLoss: 0.042559\n",
      "Train Epoch: 27 [2560/3000 (85%)]\tLoss: 0.045502\n",
      "\n",
      "Test set: Average loss: 0.2075, Accuracy: 9435/10000 (94%)\n",
      "\n",
      "Train Epoch: 28 [0/3000 (0%)]\tLoss: 0.047448\n",
      "Train Epoch: 28 [640/3000 (21%)]\tLoss: 0.043945\n",
      "Train Epoch: 28 [1280/3000 (43%)]\tLoss: 0.040435\n",
      "Train Epoch: 28 [1920/3000 (64%)]\tLoss: 0.049993\n",
      "Train Epoch: 28 [2560/3000 (85%)]\tLoss: 0.047648\n",
      "\n",
      "Test set: Average loss: 0.1982, Accuracy: 9446/10000 (94%)\n",
      "\n",
      "Train Epoch: 29 [0/3000 (0%)]\tLoss: 0.039979\n",
      "Train Epoch: 29 [640/3000 (21%)]\tLoss: 0.047498\n",
      "Train Epoch: 29 [1280/3000 (43%)]\tLoss: 0.039889\n",
      "Train Epoch: 29 [1920/3000 (64%)]\tLoss: 0.044554\n",
      "Train Epoch: 29 [2560/3000 (85%)]\tLoss: 0.043250\n",
      "\n",
      "Test set: Average loss: 0.1947, Accuracy: 9459/10000 (95%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 30):\n",
    "    train(epoch)\n",
    "    test(epoch, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
