{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pickle \n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainset_labeled = pickle.load(open(\"train_labeled.p\", \"rb\")) \n",
    "train_loader  = torch.utils.data.DataLoader(trainset_labeled, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "validset = pickle.load(open(\"validation.p\", \"rb\"))\n",
    "valid_loader = torch.utils.data.DataLoader(validset, batch_size=64, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#utility functions\n",
    "def rand(x, level = 1e-3):\n",
    "    return x + torch.randn(x.size()) * level\n",
    "\n",
    "def norm_weights_2d(size):\n",
    "    return BatchNorm2d(nn.Linear(size))\n",
    "def norm_weights_1d(size):\n",
    "    return BatchNorm2d(nn.Linear(size))\n",
    "\n",
    "def reluN(x, level = 1e-3):\n",
    "    y = x + torch.randn(x.size()) * level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def norm(size, dim = 1):\n",
    "    if dim == 1:\n",
    "        return torch.nn.BatchNorm1d(size)\n",
    "    else:\n",
    "        return torch.nn.BatchNorm2d(size)\n",
    "def linear(indim, outdim):\n",
    "    return nn.Linear(indim, outdim)\n",
    "def var(tens):\n",
    "    # The whole torch.nn module uses Parameter objects, which are essentially wrappers\n",
    "    # of autograd Variables.\n",
    "    return torch.nn.Parameter(tens)\n",
    "def randn(size):\n",
    "    return torch.randn(size)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers = [1000, 600, 300, 10]\n",
    "        layers = self.layers\n",
    "        self.encs = [linear(28*28, layers[0])]\n",
    "        self.lats = []\n",
    "        self.encnorms = []\n",
    "        self.decs = []\n",
    "        self.decnorms = []\n",
    "        \n",
    "        def _add_module_to_list(name, module, module_list):\n",
    "            self.__setattr__(name, module)\n",
    "            module_list.append(module)\n",
    "\n",
    "        for idx, l in enumerate(layers):\n",
    "            # When setting any attribute in a Module, PyTorch will try to figure out\n",
    "            # what kind of value it is: if it is a Parameter or a Module, PyTorch\n",
    "            # will automatically add the Parameter or Module into the parameter/module\n",
    "            # list.  I'm not sure if this is a good pattern in Python because adding\n",
    "            # modules invisibly like this would cause problems if one wants to maintain\n",
    "            # modules in list attributes for dynamicity (like here).  I would rather\n",
    "            # require the developers to always explicitly add modules with \"add_module()\"\n",
    "            # (I'm more convinced to avoid deriving from Module in general).\n",
    "            _add_module_to_list('_encnorm%d' % idx, norm(l), self.encnorms)\n",
    "            if idx < len(layers) - 1:\n",
    "                _add_module_to_list('_lat%d' % idx, linear(l, l), self.lats)\n",
    "                _add_module_to_list('_enc%d' % idx, linear(l, layers[idx+1]), self.encs)\n",
    "                _add_module_to_list('_dec%d' % idx, linear(layers[idx+1], l), self.decs)\n",
    "                _add_module_to_list('_decnorm%d' % idx, norm(l), self.decnorms)\n",
    "        self.batch_size = 64\n",
    "        self.weights = [self.encs, self.lats, self.decs]\n",
    "        \n",
    "\n",
    "    def forward(self, x, v = 0, noise=1e-3):\n",
    "        self.eps = noise\n",
    "        self.batch_size = x.size()[0]\n",
    "        bs = self.batch_size\n",
    "        #enc= F.relu(self.enc4(F.relu(self.enc3(F.relu(self.enc2(F.relu(self.enc1(x))))))))\n",
    "        corrupted = []\n",
    "        corruptedout = []\n",
    "        for idx, l in enumerate(self.layers):\n",
    "            if idx == 0:\n",
    "                corrupted.append(self.encnorms[idx](self.encs[idx](x)))\n",
    "            else:\n",
    "                corrupted.append(self.encnorms[idx](self.encs[idx](corruptedout[-1])))\n",
    "            \n",
    "            corruptedout.append(F.relu(corrupted[-1] + self.encnorms[idx].weight.unsqueeze(0).expand(\n",
    "                    bs, l) * var(randn(corrupted[-1].size()) * self.eps) +\n",
    "                    self.encnorms[idx].bias.unsqueeze(0).expand(bs, l)))\n",
    "            \n",
    "        encout = F.softmax(corruptedout[-1])\n",
    "        \n",
    "        clean = [x]\n",
    "        for norm, enc in zip(self.encnorms, self.encs):\n",
    "            clean.append(F.relu(norm(enc(clean[-1]))))\n",
    "        \n",
    "        decout = [encout]\n",
    "        decin = []\n",
    "        for idx in range(len(self.layers) - 2, -1, -1):\n",
    "            dec = self.decs[idx]\n",
    "            lat = self.lats[idx]\n",
    "            decnorm = self.decnorms[idx]\n",
    "            decin.append(dec(decout[-1]) + lat(corruptedout[idx]))\n",
    "            decout.append(F.relu(decin[-1] + decnorm.weight.unsqueeze(0).expand(bs, self.layers[idx]) *\n",
    "                    var(randn(decin[-1].size()) * self.eps) + \\\n",
    "                    decnorm.bias.unsqueeze(0).expand(bs, self.layers[idx])))\n",
    "        weight_reg = 0\n",
    "        for w_list in self.weights:\n",
    "            for w in w_list:\n",
    "                weight_reg += (w.weight**2).mean()/100\n",
    "        \n",
    "        yhat = F.log_softmax(corruptedout[-1])\n",
    "        \n",
    "        encode_err = 0\n",
    "        enc_weight = 1\n",
    "        enc_decay = .5\n",
    "        for c, d in zip(clean[-2::-1], decin):\n",
    "            encode_err += enc_weight * ((c - d)**2).mean()\n",
    "            enc_weight *= enc_decay\n",
    "        return yhat, weight_reg + encode_err\n",
    "        \n",
    "\n",
    "model = Net()\n",
    "params = list(model.parameters())\n",
    "\n",
    "if 0:\n",
    "    print(model)\n",
    "    print('Models has {} learnable paramater:'.format(len(params)))\n",
    "    [print('parameter {} has a size of {}'.format(i+1, params[i].size())) for i in range(len(params))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Linear (600 -> 1000), Linear (300 -> 600), Linear (10 -> 300)]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target) # Wrap them in Variable \n",
    "        optimizer.zero_grad() # Zero the parameter gradients\n",
    "        data = data.view(data.size()[0], 28*28)\n",
    "        outputs = model(data,noise=1e-2) # Forward \n",
    "        output = outputs[0]\n",
    "        US = outputs[1]\n",
    "        loss = F.nll_loss(output, target) + US*.1\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(epoch, valid_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in valid_loader:\n",
    "\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        data = data.view(data.size()[0], 28*28)\n",
    "        outputs = model(data, noise = 0)\n",
    "        output = outputs[0]\n",
    "        test_loss += F.nll_loss(output, target).data[0]\n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    test_loss /= len(valid_loader) # loss function already averages over batch size\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(valid_loader.dataset),\n",
    "        100. * correct / len(valid_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/3000 (0%)]\tLoss: 0.073967\n",
      "Train Epoch: 1 [640/3000 (21%)]\tLoss: 0.920979\n",
      "Train Epoch: 1 [1280/3000 (43%)]\tLoss: 0.631847\n",
      "Train Epoch: 1 [1920/3000 (64%)]\tLoss: 0.405158\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 30):\n",
    "    train(epoch)\n",
    "    test(epoch, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
