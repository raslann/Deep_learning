{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pickle \n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import torch.optim as optim\n",
    "#from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainset_labeled = pickle.load(open(\"train_labeled.p\", \"rb\")) \n",
    "train_loader  = torch.utils.data.DataLoader(trainset_labeled, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "validset = pickle.load(open(\"validation.p\", \"rb\"))\n",
    "valid_loader = torch.utils.data.DataLoader(validset, batch_size=64, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_numpy = trainset_labeled.train_data.numpy()\n",
    "#train_numpy = np.concatenate((train_numpy, train_numpy),0)\n",
    "train_labels_numpy = trainset_labeled.train_labels.numpy()\n",
    "#train_labels_numpy = np.concatenate((train_labels_numpy, train_labels_numpy),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       ..., \n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 740,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#np.zeros((3000,np.max(0,vshift),28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_shifted = copy.copy(train_numpy)\n",
    "train_labeled_shifted = copy.copy(train_labels_numpy)\n",
    "train_numpy.shape\n",
    "for hshift in range(-4,6,2):\n",
    "    for vshift in range(-4,6,2):\n",
    "        next_shift = np.concatenate((np.zeros((3000,np.max((0,vshift)),28)),\n",
    "                                      train_numpy[:,np.max((0,vshift)):np.min((28,28+vshift)),:],\n",
    "                                      np.zeros((3000,np.max((0,-1*vshift)),28))),1)\n",
    "        \n",
    "        next_shift = np.concatenate((np.zeros((3000,28,np.max((0,hshift)))),\n",
    "                                      next_shift[:,:,np.max((0,hshift)):np.min((28,28+hshift))],\n",
    "                                      np.zeros((3000,28,np.max((0,-1*hshift))))),2)\n",
    "        \n",
    "        train_shifted = np.concatenate((train_shifted, next_shift))\n",
    "        train_labeled_shifted = np.concatenate((train_labels_numpy, train_labeled_shifted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78000, 28, 28)"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_shifted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000,)"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_numpy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_shifted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_shifted[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.from_numpy(train_shifted)\n",
    "trainset_labeled.train_data = torch.from_numpy(train_shifted)\n",
    "trainset_labeled.train_labels = torch.LongTensor(train_labeled_shifted)\n",
    "\n",
    "trainset_labeled.k = len(trainset_labeled.train_data)\n",
    "\n",
    "train_loader  = torch.utils.data.DataLoader(trainset_labeled, batch_size=64, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_check_exists',\n",
       " 'download',\n",
       " 'k',\n",
       " 'processed_folder',\n",
       " 'raw_folder',\n",
       " 'root',\n",
       " 'target_transform',\n",
       " 'test_file',\n",
       " 'train',\n",
       " 'train_data',\n",
       " 'train_labels',\n",
       " 'training_file',\n",
       " 'transform',\n",
       " 'urls']"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(trainset_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([78000, 28, 28])"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset_labeled.train_data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([78000, 28, 28])"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset_labeled.train_data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dir(trainset_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#utility functions\n",
    "def rand(x, level = 1e-3):\n",
    "    return x + torch.randn(x.size()) * level\n",
    "\n",
    "def norm_weights_2d(size):\n",
    "    return BatchNorm2d(nn.Linear(size))\n",
    "def norm_weights_1d(size):\n",
    "    return BatchNorm2d(nn.Linear(size))\n",
    "\n",
    "def reluN(x, level = 1e-3):\n",
    "    y = x + torch.randn(x.size()) * level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def norm(size, dim = 1):\n",
    "    if dim == 1:\n",
    "        return torch.nn.BatchNorm1d(size)\n",
    "    else:\n",
    "        return torch.nn.BatchNorm2d(size)\n",
    "\n",
    "    \n",
    "def vect_std(tens):\n",
    "    tens_mean = tens.mean(0)\n",
    "    length = tens.size()[0]\n",
    "    return  ((tens - broad(tens_mean, length))**2).sum(0).sqrt()/ (length-1)\n",
    "\n",
    "def linear(indim, outdim):\n",
    "    return nn.Linear(indim, outdim)\n",
    "def var(tens):\n",
    "    # The whole torch.nn module uses Parameter objects, which are essentially wrappers\n",
    "    # of autograd Variables.\n",
    "    return torch.nn.Parameter(tens)\n",
    "def randn(size):\n",
    "    return torch.randn(size)\n",
    "def matmul(w1, w2):\n",
    "    return w1.mm(w2)\n",
    "\n",
    "def broad(tens, bs):\n",
    "    size = list(tens.size())\n",
    "    return tens.unsqueeze(0).expand(*([bs] + size))\n",
    "\n",
    "def expand(tens, bs):\n",
    "    size = list(tens.size())[1:]\n",
    "    return tens.expand(*([bs] + size))\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def u(self, v, z):\n",
    "        \n",
    "        return self.normalize_vector(v(z))\n",
    "    \n",
    "    def normalize_vector(self, vect):\n",
    "        bs = vect.size()[0]\n",
    "        vectmean = vect.mean(0)\n",
    "        vectstd = vect_std(vect)\n",
    "        vznorm = (vect - broad(vectmean, bs)) / broad(vectstd, bs)\n",
    "        return vznorm\n",
    "    \n",
    "    def normalize_vector_noise(self, vect, noise):\n",
    "        bs = vect.size()[0]\n",
    "        vectmean = vect.mean(0)\n",
    "        vectstd = vect_std(vect)\n",
    "        vznorm = (vect - broad(vectmean, bs)) / broad(vectstd, bs)\n",
    "        return vznorm + noise\n",
    "    \n",
    "    \n",
    "        \n",
    "    def mu_v(self, u, a1, a2, a3, a4, a5):\n",
    "        bs = u.size()[0]\n",
    "        a_size = a1.size()[0]\n",
    "        return (a1.unsqueeze(0).expand(bs,a_size) * \n",
    "            torch.sigmoid(a2.unsqueeze(0).expand(bs,a_size) * u + a3.unsqueeze(0).expand(bs,a_size)) + \n",
    "            a4.unsqueeze(0).expand(bs,a_size) * u + a5.unsqueeze(0).expand(bs,a_size))\n",
    "        \n",
    "    #z is lateral, u is vetrtical from above\n",
    "    def g(self, z, u, a1, a2, a3, a4, a5, a6, a7, a8, a9, a10):\n",
    "        return (z - self.mu_v(u, a1, a2, a3, a4, a5)) * self.mu_v(u, a6, a7, a8, a9, a10) + \\\n",
    "            self.mu_v(u, a1, a2, a3, a4, a5)\n",
    "        \n",
    "        \n",
    "    def __init__(self, layers = [100, 100, 100]):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers = layers\n",
    "        layers = self.layers\n",
    "        self.encs = [linear(28*28, layers[0])]\n",
    "        self.lats = []\n",
    "        self.encnorms = []\n",
    "        self.purenorms = []\n",
    "        self.decs = []\n",
    "        self.decnorms = []\n",
    "        self.a1s = []\n",
    "        self.a2s = []\n",
    "        self.a3s = []\n",
    "        self.a4s = []\n",
    "        self.a5s = []\n",
    "        self.a6s = []\n",
    "        self.a7s = []\n",
    "        self.a8s = []\n",
    "        self.a9s = []\n",
    "        self.a10s = []\n",
    "        \n",
    "        def _add_module_to_list(name, module, module_list):\n",
    "            self.__setattr__(name, module)\n",
    "            module_list.append(module)\n",
    "\n",
    "        for idx, l in enumerate(layers):\n",
    "            # When setting any attribute in a Module, PyTorch will try to figure out\n",
    "            # what kind of value it is: if it is a Parameter or a Module, PyTorch\n",
    "            # will automatically add the Parameter or Module into the parameter/module\n",
    "            # list.  I'm not sure if this is a good pattern in Python because adding\n",
    "            # modules invisibly like this would cause problems if one wants to maintain\n",
    "            # modules in list attributes for dynamicity (like here).  I would rather\n",
    "            # require the developers to always explicitly add modules with \"add_module()\"\n",
    "            # (I'm more convinced to avoid deriving from Module in general).\n",
    "            _add_module_to_list('_encnorm%d' % idx, norm(l), self.encnorms)\n",
    "            _add_module_to_list('_purenorm%d' % idx, norm(l), self.purenorms)\n",
    "\n",
    "            _add_module_to_list('_a1s%d' % idx, nn.Parameter(torch.randn(l)*1e-3), self.a1s)\n",
    "            _add_module_to_list('_a2s%d' % idx, nn.Parameter(torch.randn(l)*1e-3), self.a2s)\n",
    "            _add_module_to_list('_a3s%d' % idx, nn.Parameter(torch.randn(l)*1e-3), self.a3s)\n",
    "            _add_module_to_list('_a4s%d' % idx, nn.Parameter(torch.randn(l)*1e-3), self.a4s)\n",
    "            _add_module_to_list('_a5s%d' % idx, nn.Parameter(torch.randn(l)*1e-3), self.a5s)\n",
    "            _add_module_to_list('_a6s%d' % idx, nn.Parameter(torch.randn(l)*1e-3), self.a6s)\n",
    "            _add_module_to_list('_a7s%d' % idx, nn.Parameter(torch.randn(l)*1e-3), self.a7s)\n",
    "            _add_module_to_list('_a8s%d' % idx, nn.Parameter(torch.randn(l)*1e-3), self.a8s)\n",
    "            _add_module_to_list('_a9s%d' % idx, nn.Parameter(torch.randn(l)*1e-3), self.a9s)\n",
    "            _add_module_to_list('_a10s%d' % idx, nn.Parameter(torch.randn(l)*1e-3), self.a10s)\n",
    "            \n",
    "            if idx < len(layers) - 1:\n",
    "                _add_module_to_list('_lat%d' % idx, linear(l, l), self.lats)\n",
    "                _add_module_to_list('_enc%d' % idx, linear(l, layers[idx+1]), self.encs)\n",
    "                _add_module_to_list('_dec%d' % idx, linear(layers[idx+1], l), self.decs)\n",
    "                _add_module_to_list('_decnorm%d' % idx, norm(l), self.decnorms)\n",
    "                \n",
    "        self.batch_size = 64\n",
    "        self.weights = [self.encs, self.decs]\n",
    "        \n",
    "\n",
    "    def forward(self, x, v = 0, noise=1e-3):\n",
    "        self.eps = noise\n",
    "        self.batch_size = x.size()[0]\n",
    "        bs = self.batch_size\n",
    "        #enc= F.relu(self.enc4(F.relu(self.enc3(F.relu(self.enc2(F.relu(self.enc1(x))))))))\n",
    "        corrupted = []\n",
    "        corruptedout = []\n",
    "        corrupted_pre_scaled = []\n",
    "        for idx, l in enumerate(self.layers):\n",
    "            if idx == 0:\n",
    "                corrupted.append(self.encnorms[idx](self.encs[idx](x)))\n",
    "                cur_noise = var(randn(corrupted[-1].size()) * self.eps)\n",
    "                corrupted_pre_scaled.append(self.normalize_vector_noise(self.encs[idx](x), cur_noise))\n",
    "            else:\n",
    "                corrupted.append(self.encnorms[idx](self.encs[idx](corruptedout[-1])))\n",
    "                cur_noise = var(randn(corrupted[-1].size()) * self.eps)\n",
    "                corrupted_pre_scaled.append(self.normalize_vector_noise(self.encs[idx](corruptedout[-1]), cur_noise))\n",
    "                \n",
    "            corruptedout.append(F.relu(corrupted[-1] + self.encnorms[idx].weight.unsqueeze(0).expand(\n",
    "                    bs, l) * cur_noise +\n",
    "                    self.encnorms[idx].bias.unsqueeze(0).expand(bs, l)))\n",
    "            \n",
    "        encout = F.softmax(corruptedout[-1])\n",
    "        \n",
    "        clean = [x]\n",
    "        clean_pre_scaled = []\n",
    "        mu_l = []\n",
    "        std_l = []\n",
    "        for norm, enc in zip(self.encnorms, self.encs):\n",
    "            clean_pre_scaled.append(self.normalize_vector(enc(clean[-1])))\n",
    "            mu_l.append(clean_pre_scaled[-1].mean(0))\n",
    "            std_l.append(vect_std(clean_pre_scaled[-1]))\n",
    "            clean.append(F.relu(norm(enc(clean[-1]))))\n",
    "        \n",
    "        #Decout starts as the output. For future iterations down decoding path,\n",
    "        #Decout[-1] will contain V*Z of above decoder\n",
    "        decout = [encout]\n",
    "        z_hats = []\n",
    "        for idx in range(len(self.layers) - 1, -1, -1):\n",
    "            a1, a2, a3, a4, a5, a6, a7, a8, a9, a10 = self.a1s[idx], self.a2s[idx], self.a3s[idx], \\\n",
    "                self.a4s[idx], self.a5s[idx], self.a6s[idx], self.a7s[idx], \\\n",
    "                self.a8s[idx], self.a9s[idx], self.a10s[idx]\n",
    "            \n",
    "            decin = decout[-1]\n",
    "            U_l = self.normalize_vector(decin)\n",
    "            z_tild = corrupted_pre_scaled[idx]\n",
    "            \n",
    "            z_hat = self.g(z_tild, U_l, a1, a2, a3, a4, a5, a6, a7, a8, a9, a10)\n",
    "            z_hats.append((z_hat - expand(mu_l[idx], bs)) / expand(std_l[idx], bs))\n",
    "            \n",
    "            if idx > 0:\n",
    "                v = self.decs[idx-1]\n",
    "                decout.append(v(z_hat))\n",
    "            \n",
    "        weight_reg = 0\n",
    "        for w_list in self.weights:\n",
    "            for w in w_list:\n",
    "                weight_reg += (w.weight**2).mean()/1000\n",
    "        \n",
    "        yhat = F.log_softmax(corruptedout[-1])\n",
    "        \n",
    "        encode_err = 0\n",
    "        enc_weight = .02\n",
    "        enc_decay = .2\n",
    "        for c, d in zip(clean_pre_scaled[-2::-1], z_hats[1:]):\n",
    "            encode_err += enc_weight * ((c - d)**2).mean()\n",
    "            enc_weight *= enc_decay\n",
    "        return yhat, encode_err, weight_reg, encode_err\n",
    "        \n",
    "\n",
    "model = Net([400, 100, 10])\n",
    "params = list(model.parameters())\n",
    "\n",
    "if 0:\n",
    "    print(model)\n",
    "    print('Models has {} learnable paramater:'.format(len(params)))\n",
    "    [print('parameter {} has a size of {}'.format(i+1, params[i].size())) for i in range(len(params))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001"
      ]
     },
     "execution_count": 743,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.state_dict()['param_groups'][0]['weight_decay']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target) # Wrap them in Variable \n",
    "        optimizer.zero_grad() # Zero the parameter gradients\n",
    "        data = data.view(data.size()[0], 28*28)\n",
    "        outputs = model(data,noise=1e-2) # Forward\n",
    "        output = outputs[0]\n",
    "        US = outputs[1]\n",
    "        loss = F.nll_loss(output, target) + US\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    \n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target) # Wrap them in Variable\n",
    "        optimizer.zero_grad() # Zero the parameter gradients\n",
    "        data = data.view(data.size()[0], 28*28)\n",
    "        #data2 = data.clone()\n",
    "        '''\n",
    "        for flips in range(3):\n",
    "            if flips == 0:\n",
    "                hshift = np.random.randint(10)\n",
    "                vshift = np.random.randint(10)\n",
    "                data2 = copy.deepcopy(data.resize_as(torch.FloatTensor(data.size()[0], 28, 28)))\n",
    "                #data2 = data.resize_as(torch.FloatTensor(data.size()[0], 28, 28))\n",
    "                if 1:\n",
    "                    data2[:,:,hshift:] = data2[:,:,:28-hshift]\n",
    "                    data2[:,:,:hshift] = 0\n",
    "                    #data2 = torch.cat((torch.FloatTensor(data.size()[0], 28,hshift), data2[:,:,hshift:]))\n",
    "            if flips == 1:\n",
    "                data2 = data.resize_as(torch.FloatTensor(data.size()[0], 28, 28))\n",
    "                data2 = data2.transpose(1,2)\n",
    "                data2 = data2.resize_as(torch.FloatTensor(data.size()[0], 784))\n",
    "            if flips == 2:\n",
    "                data2 = data\n",
    "        '''\n",
    "        outputs = model(data,noise=2e-1) # Forward \n",
    "        #print('yhat size',outputs[0].size())\n",
    "        output = outputs[0]\n",
    "        US = outputs[1]\n",
    "        loss = F.nll_loss(output, target) + US*.1\n",
    "        #print('loss', loss)\n",
    "        #print('yhat', output[0])\n",
    "        #print('unsupervised loss', US)\n",
    "        #print('weight reg', outputs[2])\n",
    "        #print('encoder error', outputs[3])\n",
    "        #best_optim = copy.copy(optimizer)\n",
    "        loss.backward()\n",
    "        _maxp = 0\n",
    "        nan_vals = False\n",
    "        #print(data)\n",
    "        for p in optimizer.state_dict()['param_groups'][0]['params']:\n",
    "            print(_maxp)\n",
    "            #print('abs',p.abs())\n",
    "            print(type(p.abs()))\n",
    "            print('max:',p.abs().max())\n",
    "            print('data:',p.abs().max().data)\n",
    "            print('numpy:',p.abs().max().data.numpy())\n",
    "            _maxp = max(_maxp, p.abs().max().data.numpy())\n",
    "            p.grad.data.clamp_(-.01, .01)\n",
    "        #print(_maxp)\n",
    "        #return\n",
    "        if not nan_Vals:\n",
    "            optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78000"
      ]
     },
     "execution_count": 756,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9546216325095163\n"
     ]
    }
   ],
   "source": [
    "lshift = np.random.random()\n",
    "print(lshift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test(epoch, valid_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in valid_loader:\n",
    "            data, target = Variable(data, volatile=True), Variable(target)\n",
    "            data = data.view(data.size()[0], 28*28)\n",
    "            outputs = model(data, noise = 0)\n",
    "            output = outputs[0]\n",
    "            test_loss += F.nll_loss(output, target).data[0]\n",
    "            pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    test_loss /= len(valid_loader) # loss function already averages over batch size\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(valid_loader.dataset),\n",
    "        100. * correct / len(valid_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/78000 (0%)]\tLoss: 2.206915\n",
      "Train Epoch: 1 [640/78000 (1%)]\tLoss: 2.266315\n",
      "Train Epoch: 1 [1280/78000 (2%)]\tLoss: 2.226717\n",
      "Train Epoch: 1 [1920/78000 (2%)]\tLoss: 2.281904\n",
      "Train Epoch: 1 [2560/78000 (3%)]\tLoss: 2.212449\n",
      "Train Epoch: 1 [3200/78000 (4%)]\tLoss: 2.209245\n",
      "Train Epoch: 1 [3840/78000 (5%)]\tLoss: 2.216330\n",
      "Train Epoch: 1 [4480/78000 (6%)]\tLoss: 2.195629\n",
      "Train Epoch: 1 [5120/78000 (7%)]\tLoss: 2.150784\n",
      "Train Epoch: 1 [5760/78000 (7%)]\tLoss: 2.226138\n",
      "Train Epoch: 1 [6400/78000 (8%)]\tLoss: 2.213660\n",
      "Train Epoch: 1 [7040/78000 (9%)]\tLoss: 2.268196\n",
      "Train Epoch: 1 [7680/78000 (10%)]\tLoss: 2.235765\n",
      "Train Epoch: 1 [8320/78000 (11%)]\tLoss: 2.221062\n",
      "Train Epoch: 1 [8960/78000 (11%)]\tLoss: 2.214576\n",
      "Train Epoch: 1 [9600/78000 (12%)]\tLoss: nan\n",
      "Train Epoch: 1 [10240/78000 (13%)]\tLoss: nan\n",
      "Train Epoch: 1 [10880/78000 (14%)]\tLoss: nan\n",
      "Train Epoch: 1 [11520/78000 (15%)]\tLoss: nan\n",
      "Train Epoch: 1 [12160/78000 (16%)]\tLoss: nan\n",
      "Train Epoch: 1 [12800/78000 (16%)]\tLoss: nan\n",
      "Train Epoch: 1 [13440/78000 (17%)]\tLoss: nan\n",
      "Train Epoch: 1 [14080/78000 (18%)]\tLoss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-350:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [14720/78000 (19%)]\tLoss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-349:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lee/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lee/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lee/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lee/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lee/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 26, in _worker_loop\n",
      "    r = index_queue.get()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-759-c8a36916e533>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-755-7a89d67bb966>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m#print('weight reg', outputs[2])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m#print('encoder error', outputs[3])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0m_maxp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m#print(data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lee/anaconda3/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'backward should be called only on a scalar (i.e. 1-element tensor) or with gradient w.r.t. the variable'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_as_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execution_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lee/anaconda3/lib/python3.5/site-packages/torch/nn/_functions/linear.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad_output)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mgrad_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneeds_input_grad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mgrad_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneeds_input_grad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mgrad_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/lee/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/lee/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 26, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/lee/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/lee/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/lee/anaconda3/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/lee/anaconda3/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/lee/anaconda3/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 30):\n",
    "    train(epoch)\n",
    "    test(epoch, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'RandomHorizontalFlip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-760-67b2031bb4e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomHorizontalFlip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'RandomHorizontalFlip'"
     ]
    }
   ],
   "source": [
    "a.RandomHorizontalFlip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'arr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-480-c10a5464e1f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'arr' is not defined"
     ]
    }
   ],
   "source": [
    "torch.from_numpy(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arr = np.ones(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-482-07016687bdda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masNumpyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "a.asNumpyarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-483-60b725f10c9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = list(range(784,-1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dataarr[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'image'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-486-5350a1bf76ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'image'"
     ]
    }
   ],
   "source": [
    "torch.image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataarr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-487-f2af4eacecd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataarr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dataarr' is not defined"
     ]
    }
   ],
   "source": [
    "dataarr.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'torchvision.transforms' from '/home/lee/anaconda3/lib/python3.5/site-packages/torchvision-0.1.6-py3.5.egg/torchvision/transforms.py'>"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n",
      "Train Epoch: 1 [0/78000 (0%)]\tLoss: 17.828180\n",
      "torch.Size([64, 10])\n",
      "torch.Size([64, 10])\n",
      "torch.Size([64, 10])\n",
      "torch.Size([64, 10])\n",
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    data, target = Variable(data), Variable(target) # Wrap them in Variable\n",
    "    optimizer.zero_grad() # Zero the parameter gradients\n",
    "    data = data.view(data.size()[0], 28*28)\n",
    "    #data2 = data.clone()\n",
    "    '''\n",
    "    for flips in range(3):\n",
    "        if flips == 0:\n",
    "            hshift = np.random.randint(10)\n",
    "            vshift = np.random.randint(10)\n",
    "            data2 = copy.deepcopy(data.resize_as(torch.FloatTensor(data.size()[0], 28, 28)))\n",
    "            #data2 = data.resize_as(torch.FloatTensor(data.size()[0], 28, 28))\n",
    "            if 1:\n",
    "                data2[:,:,hshift:] = data2[:,:,:28-hshift]\n",
    "                data2[:,:,:hshift] = 0\n",
    "                #data2 = torch.cat((torch.FloatTensor(data.size()[0], 28,hshift), data2[:,:,hshift:]))\n",
    "        if flips == 1:\n",
    "            data2 = data.resize_as(torch.FloatTensor(data.size()[0], 28, 28))\n",
    "            data2 = data2.transpose(1,2)\n",
    "            data2 = data2.resize_as(torch.FloatTensor(data.size()[0], 784))\n",
    "        if flips == 2:\n",
    "            data2 = data\n",
    "    '''\n",
    "    outputs = model(data,noise=2e-1) # Forward \n",
    "    #print(outputs[0].size())\n",
    "    output = outputs[0]\n",
    "    US = outputs[1]\n",
    "    loss = F.nll_loss(output, target) + US*.1\n",
    "    #print('loss', loss)\n",
    "    #print('yhat', output)\n",
    "    #print('unsupervised loss', US)\n",
    "    #print('weight reg', outputs[2])\n",
    "    #print('encoder error', outputs[3])\n",
    "    if np.isnan(outputs[2].data.numpy()) or np.isnan(outputs[3].data.numpy()):\n",
    "        break\n",
    "    loss.backward() \n",
    "    optimizer.step()\n",
    "    if batch_idx % 10 == 0:\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "clamp_",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-620-b59a53159bf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'param_groups'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;31m#return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lee/anaconda3/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fallthrough_methods\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: clamp_"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    data, target = Variable(data), Variable(target) # Wrap them in Variable\n",
    "    optimizer.zero_grad() # Zero the parameter gradients\n",
    "    data = data.view(data.size()[0], 28*28)\n",
    "    #data2 = data.clone()\n",
    "    '''\n",
    "    for flips in range(3):\n",
    "        if flips == 0:\n",
    "            hshift = np.random.randint(10)\n",
    "            vshift = np.random.randint(10)\n",
    "            data2 = copy.deepcopy(data.resize_as(torch.FloatTensor(data.size()[0], 28, 28)))\n",
    "            #data2 = data.resize_as(torch.FloatTensor(data.size()[0], 28, 28))\n",
    "            if 1:\n",
    "                data2[:,:,hshift:] = data2[:,:,:28-hshift]\n",
    "                data2[:,:,:hshift] = 0\n",
    "                #data2 = torch.cat((torch.FloatTensor(data.size()[0], 28,hshift), data2[:,:,hshift:]))\n",
    "        if flips == 1:\n",
    "            data2 = data.resize_as(torch.FloatTensor(data.size()[0], 28, 28))\n",
    "            data2 = data2.transpose(1,2)\n",
    "            data2 = data2.resize_as(torch.FloatTensor(data.size()[0], 784))\n",
    "        if flips == 2:\n",
    "            data2 = data\n",
    "    '''\n",
    "    outputs = model(data,noise=2e-1) # Forward \n",
    "    #print('yhat size',outputs[0].size())\n",
    "    output = outputs[0]\n",
    "    US = outputs[1]\n",
    "    loss = F.nll_loss(output, target) + US*.1\n",
    "    #print('loss', loss)\n",
    "    #print('yhat', output[0])\n",
    "    #print('unsupervised loss', US)\n",
    "    #print('weight reg', outputs[2])\n",
    "    #print('encoder error', outputs[3])\n",
    "    loss.backward()\n",
    "    for p in optimizer.state_dict()['param_groups'][0]['params']:\n",
    "        p.grad.data.clamp_(-1, 1)\n",
    "    #return\n",
    "    optimizer.step()\n",
    "    if batch_idx % 10 == 0:\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.autograd.variable.Variable"
      ]
     },
     "execution_count": 622,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
