{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pickle \n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import torch.optim as optim\n",
    "#from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded\n",
      "3000\n",
      "750\n"
     ]
    }
   ],
   "source": [
    "trainset_labeled = pickle.load(open(\"train_labeled.p\", \"rb\")) \n",
    "train_loader  = torch.utils.data.DataLoader(trainset_labeled, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "validset = pickle.load(open(\"validation.p\", \"rb\"))\n",
    "valid_loader = torch.utils.data.DataLoader(validset, batch_size=64, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_numpy = trainset_labeled.train_data.numpy()\n",
    "#train_numpy = np.concatenate((train_numpy, train_numpy),0)\n",
    "train_labels_numpy = trainset_labeled.train_labels.numpy()\n",
    "#train_labels_numpy = np.concatenate((train_labels_numpy, train_labels_numpy),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       ..., \n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "train_numpy_reshaped = train_numpy.reshape(-1, 784)\n",
    "all_black = np.max(train_numpy_reshaped,1) == 0\n",
    "print(all_black.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "254"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(train_numpy_reshaped,1).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_shifted = copy.copy(train_numpy)\n",
    "train_labeled_shifted = copy.copy(train_labels_numpy)\n",
    "train_numpy.shape\n",
    "for hshift in range(-4,6,2):\n",
    "    for vshift in range(-4,6,2):\n",
    "        next_shift = np.concatenate((np.zeros((3000,np.max((0,-1*vshift)),28)),\n",
    "                                      train_numpy[:,np.max((0,vshift)):np.min((28,28+vshift)),:],\n",
    "                                      np.zeros((3000,np.max((0,vshift)),28))),1)\n",
    "        \n",
    "        next_shift = np.concatenate((np.zeros((3000,28,np.max((0,-1*hshift)))),\n",
    "                                      next_shift[:,:,np.max((0,hshift)):np.min((28,28+hshift))],\n",
    "                                      np.zeros((3000,28,np.max((0,hshift))))),2)\n",
    "        \n",
    "        train_shifted = np.concatenate((train_shifted, next_shift))\n",
    "        train_labeled_shifted = np.concatenate((train_labels_numpy, train_labeled_shifted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78000, 784)\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "train_shifted_reshaped = train_shifted.reshape(-1, 784)\n",
    "print(train_shifted_reshaped.shape)\n",
    "all_black = np.max(train_shifted_reshaped,1) == 0\n",
    "print(all_black.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False False 255.0 0.0\n"
     ]
    }
   ],
   "source": [
    "print(np.isnan(train_shifted).any(), np.isinf(train_shifted).any(), np.max(train_shifted), np.min(train_shifted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_numpy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_shifted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_shifted[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainset_labeled = torch.utils.data.TensorDataset(\n",
    "    torch.FloatTensor(train_shifted) / 255.,\n",
    "    torch.LongTensor(train_labeled_shifted)\n",
    "    )\n",
    "\n",
    "train_loader  = torch.utils.data.DataLoader(trainset_labeled, batch_size=64, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TensorDataset' object has no attribute 'train_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-fcb3870e8319>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainset_labeled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'TensorDataset' object has no attribute 'train_data'"
     ]
    }
   ],
   "source": [
    "x = trainset_labeled.train_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__doc__',\n",
       " '__format__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__len__',\n",
       " '__module__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'data_tensor',\n",
       " 'target_tensor']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(trainset_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TensorDataset' object has no attribute 'train_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-5237fed7957f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainset_labeled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'TensorDataset' object has no attribute 'train_data'"
     ]
    }
   ],
   "source": [
    "trainset_labeled.train_data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TensorDataset' object has no attribute 'train_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-5237fed7957f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainset_labeled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'TensorDataset' object has no attribute 'train_data'"
     ]
    }
   ],
   "source": [
    "trainset_labeled.train_data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAFdCAYAAADSR9wBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f19c60a5790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as PL\n",
    "i = 5\n",
    "PL.matshow(trainset_labeled.data_tensor[i].numpy(), vmin=0, vmax=255, cmap='gray')\n",
    "print(trainset_labeled.target_tensor[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#utility functions\n",
    "def rand(x, level = 1e-3):\n",
    "    return x + torch.randn(x.size()) * level\n",
    "\n",
    "def norm_weights_2d(size):\n",
    "    return BatchNorm2d(nn.Linear(size))\n",
    "def norm_weights_1d(size):\n",
    "    return BatchNorm2d(nn.Linear(size))\n",
    "\n",
    "def reluN(x, level = 1e-3):\n",
    "    y = x + torch.randn(x.size()) * level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def norm(size, dim = 1):\n",
    "    if dim == 1:\n",
    "        return torch.nn.BatchNorm1d(size)\n",
    "    else:\n",
    "        return torch.nn.BatchNorm2d(size)\n",
    "\n",
    "    \n",
    "def vect_std(tens):\n",
    "    tens_mean = tens.mean(0)\n",
    "    length = tens.size()[0]\n",
    "    return  ((tens - broad(tens_mean, length))**2).sum(0).sqrt()/ (length-1) + 1e-8\n",
    "\n",
    "def linear(indim, outdim):\n",
    "    return nn.Linear(indim, outdim)\n",
    "def var(tens):\n",
    "    # The whole torch.nn module uses Parameter objects, which are essentially wrappers\n",
    "    # of autograd Variables.\n",
    "    return torch.nn.Parameter(tens)\n",
    "def randn(size):\n",
    "    return torch.randn(size)\n",
    "def matmul(w1, w2):\n",
    "    return w1.mm(w2)\n",
    "\n",
    "def broad(tens, bs):\n",
    "    size = list(tens.size())\n",
    "    return tens.unsqueeze(0).expand(*([bs] + size))\n",
    "\n",
    "def expand(tens, bs):\n",
    "    size = list(tens.size())[1:]\n",
    "    return tens.expand(*([bs] + size))\n",
    "\n",
    "def anynan(x):\n",
    "    return nan(x) or inf(x) or big(x)\n",
    "\n",
    "def nan(x):\n",
    "    return np.isnan(x.data.numpy()).any()\n",
    "\n",
    "def inf(x):\n",
    "    return np.isinf(x.data.numpy()).any()\n",
    "\n",
    "def big(x):\n",
    "    return (np.abs(x.data.numpy()) > 1e+5).any()\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def u(self, v, z):\n",
    "        \n",
    "        return self.normalize_vector(v(z))\n",
    "    \n",
    "    def normalize_vector(self, vect):\n",
    "        bs = vect.size()[0]\n",
    "        vectmean = vect.mean(0)\n",
    "        vectstd = vect_std(vect)\n",
    "        assert not anynan(vectmean)\n",
    "        assert not anynan(vectstd)\n",
    "        vznorm = (vect - broad(vectmean, bs)) / broad(vectstd, bs)\n",
    "        assert not anynan(vznorm)\n",
    "        return vznorm\n",
    "    \n",
    "    def normalize_vector_noise(self, vect, noise):\n",
    "        bs = vect.size()[0]\n",
    "        vectmean = vect.mean(0)\n",
    "        vectstd = vect_std(vect)\n",
    "        assert not anynan(vectmean)\n",
    "        assert not anynan(vectstd)\n",
    "        vznorm = (vect - broad(vectmean, bs)) / broad(vectstd, bs)\n",
    "        assert not anynan(vznorm)\n",
    "        return vznorm + noise\n",
    "    \n",
    "    \n",
    "        \n",
    "    def mu_v(self, u, a1, a2, a3, a4, a5):\n",
    "        bs = u.size()[0]\n",
    "        a_size = a1.size()[0]\n",
    "        return (a1.unsqueeze(0).expand(bs,a_size) * \n",
    "            torch.sigmoid(a2.unsqueeze(0).expand(bs,a_size) * u + a3.unsqueeze(0).expand(bs,a_size)) + \n",
    "            a4.unsqueeze(0).expand(bs,a_size) * u + a5.unsqueeze(0).expand(bs,a_size))\n",
    "        \n",
    "    #z is lateral, u is vetrtical from above\n",
    "    def g(self, z, u, a1, a2, a3, a4, a5, a6, a7, a8, a9, a10):\n",
    "        return (z - self.mu_v(u, a1, a2, a3, a4, a5)) * self.mu_v(u, a6, a7, a8, a9, a10) + \\\n",
    "            self.mu_v(u, a1, a2, a3, a4, a5)\n",
    "        \n",
    "        \n",
    "    def __init__(self, layers = [100, 100, 100]):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers = layers\n",
    "        layers = self.layers\n",
    "        self.encs = [linear(28*28, layers[0])]\n",
    "        self.lats = []\n",
    "        self.encnorms = []\n",
    "        self.purenorms = []\n",
    "        self.decs = []\n",
    "        self.decnorms = []\n",
    "        self.a1s = []\n",
    "        self.a2s = []\n",
    "        self.a3s = []\n",
    "        self.a4s = []\n",
    "        self.a5s = []\n",
    "        self.a6s = []\n",
    "        self.a7s = []\n",
    "        self.a8s = []\n",
    "        self.a9s = []\n",
    "        self.a10s = []\n",
    "        \n",
    "        def _add_module_to_list(name, module, module_list):\n",
    "            self.__setattr__(name, module)\n",
    "            module_list.append(module)\n",
    "\n",
    "        for idx, l in enumerate(layers):\n",
    "            # When setting any attribute in a Module, PyTorch will try to figure out\n",
    "            # what kind of value it is: if it is a Parameter or a Module, PyTorch\n",
    "            # will automatically add the Parameter or Module into the parameter/module\n",
    "            # list.  I'm not sure if this is a good pattern in Python because adding\n",
    "            # modules invisibly like this would cause problems if one wants to maintain\n",
    "            # modules in list attributes for dynamicity (like here).  I would rather\n",
    "            # require the developers to always explicitly add modules with \"add_module()\"\n",
    "            # (I'm more convinced to avoid deriving from Module in general).\n",
    "            _add_module_to_list('_encnorm%d' % idx, norm(l), self.encnorms)\n",
    "            _add_module_to_list('_purenorm%d' % idx, norm(l), self.purenorms)\n",
    "\n",
    "            _add_module_to_list('_a1s%d' % idx, nn.Parameter(torch.randn(l)*1e-3), self.a1s)\n",
    "            _add_module_to_list('_a2s%d' % idx, nn.Parameter(torch.randn(l)*1e-3), self.a2s)\n",
    "            _add_module_to_list('_a3s%d' % idx, nn.Parameter(torch.randn(l)*1e-3), self.a3s)\n",
    "            _add_module_to_list('_a4s%d' % idx, nn.Parameter(torch.randn(l)*1e-3), self.a4s)\n",
    "            _add_module_to_list('_a5s%d' % idx, nn.Parameter(torch.randn(l)*1e-3), self.a5s)\n",
    "            _add_module_to_list('_a6s%d' % idx, nn.Parameter(torch.randn(l)*1e-3), self.a6s)\n",
    "            _add_module_to_list('_a7s%d' % idx, nn.Parameter(torch.randn(l)*1e-3), self.a7s)\n",
    "            _add_module_to_list('_a8s%d' % idx, nn.Parameter(torch.randn(l)*1e-3), self.a8s)\n",
    "            _add_module_to_list('_a9s%d' % idx, nn.Parameter(torch.randn(l)*1e-3), self.a9s)\n",
    "            _add_module_to_list('_a10s%d' % idx, nn.Parameter(torch.randn(l)*1e-3), self.a10s)\n",
    "            \n",
    "            if idx < len(layers) - 1:\n",
    "                _add_module_to_list('_lat%d' % idx, linear(l, l), self.lats)\n",
    "                _add_module_to_list('_enc%d' % idx, linear(l, layers[idx+1]), self.encs)\n",
    "                _add_module_to_list('_dec%d' % idx, linear(layers[idx+1], l), self.decs)\n",
    "                _add_module_to_list('_decnorm%d' % idx, norm(l), self.decnorms)\n",
    "                \n",
    "        self.batch_size = 64\n",
    "        self.weights = [self.encs, self.decs]\n",
    "        \n",
    "\n",
    "    def forward(self, x, v = 0, noise=1e-3):\n",
    "        self.eps = noise\n",
    "        self.batch_size = x.size()[0]\n",
    "        bs = self.batch_size\n",
    "        #enc= F.relu(self.enc4(F.relu(self.enc3(F.relu(self.enc2(F.relu(self.enc1(x))))))))\n",
    "        corrupted = []\n",
    "        corruptedout = []\n",
    "        corrupted_pre_scaled = []\n",
    "        for idx, l in enumerate(self.layers):\n",
    "            if idx == 0:\n",
    "                corrupted.append(self.encnorms[idx](self.encs[idx](x)))\n",
    "                cur_noise = var(randn(corrupted[-1].size()) * self.eps)\n",
    "                corrupted_pre_scaled.append(self.normalize_vector_noise(self.encs[idx](x), cur_noise))\n",
    "            else:\n",
    "                corrupted.append(self.encnorms[idx](self.encs[idx](corruptedout[-1])))\n",
    "                cur_noise = var(randn(corrupted[-1].size()) * self.eps)\n",
    "                corrupted_pre_scaled.append(self.normalize_vector_noise(self.encs[idx](corruptedout[-1]), cur_noise))\n",
    "                \n",
    "            corruptedout.append(F.relu(corrupted[-1] + self.encnorms[idx].weight.unsqueeze(0).expand(\n",
    "                    bs, l) * cur_noise +\n",
    "                    self.encnorms[idx].bias.unsqueeze(0).expand(bs, l)))\n",
    "            \n",
    "        encout = F.softmax(corruptedout[-1])\n",
    "        \n",
    "        clean = [x]\n",
    "        clean_pre_scaled = []\n",
    "        mu_l = []\n",
    "        std_l = []\n",
    "        for norm, enc in zip(self.encnorms, self.encs):\n",
    "            clean_pre_scaled.append(self.normalize_vector(enc(clean[-1])))\n",
    "            mu_l.append(clean_pre_scaled[-1].mean(0))\n",
    "            std_l.append(vect_std(clean_pre_scaled[-1]))\n",
    "            clean.append(F.relu(norm(enc(clean[-1]))))\n",
    "        \n",
    "        #Decout starts as the output. For future iterations down decoding path,\n",
    "        #Decout[-1] will contain V*Z of above decoder\n",
    "        decout = [encout]\n",
    "        z_hats = []\n",
    "        for idx in range(len(self.layers) - 1, -1, -1):\n",
    "            a1, a2, a3, a4, a5, a6, a7, a8, a9, a10 = self.a1s[idx], self.a2s[idx], self.a3s[idx], \\\n",
    "                self.a4s[idx], self.a5s[idx], self.a6s[idx], self.a7s[idx], \\\n",
    "                self.a8s[idx], self.a9s[idx], self.a10s[idx]\n",
    "            \n",
    "            decin = decout[-1]\n",
    "            U_l = self.normalize_vector(decin)\n",
    "            z_tild = corrupted_pre_scaled[idx]\n",
    "            \n",
    "            z_hat = self.g(z_tild, U_l, a1, a2, a3, a4, a5, a6, a7, a8, a9, a10)\n",
    "            z_hats.append((z_hat - expand(mu_l[idx], bs)) / expand(std_l[idx], bs))\n",
    "            \n",
    "            if idx > 0:\n",
    "                v = self.decs[idx-1]\n",
    "                decout.append(v(z_hat))\n",
    "            \n",
    "        weight_reg = 0\n",
    "        for w_list in self.weights:\n",
    "            for w in w_list:\n",
    "                weight_reg += (w.weight**2).mean()/1000\n",
    "        \n",
    "        yhat = F.log_softmax(corruptedout[-1])\n",
    "        \n",
    "        encode_err = 0\n",
    "        enc_weight = .02\n",
    "        enc_decay = .2\n",
    "        for c, d in zip(clean_pre_scaled[-2::-1], z_hats[1:]):\n",
    "            encode_err += enc_weight * ((c - d)**2).mean()\n",
    "            enc_weight *= enc_decay\n",
    "        return yhat, encode_err, weight_reg, encode_err\n",
    "        \n",
    "\n",
    "model = Net([400, 100, 10])\n",
    "params = list(model.parameters())\n",
    "\n",
    "if 0:\n",
    "    print(model)\n",
    "    print('Models has {} learnable paramater:'.format(len(params)))\n",
    "    [print('parameter {} has a size of {}'.format(i+1, params[i].size())) for i in range(len(params))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.state_dict()['param_groups'][0]['weight_decay']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target) # Wrap them in Variable \n",
    "        optimizer.zero_grad() # Zero the parameter gradients\n",
    "        data = data.view(data.size()[0], 28*28)\n",
    "        outputs = model(data,noise=1e-2) # Forward\n",
    "        output = outputs[0]\n",
    "        US = outputs[1]\n",
    "        loss = F.nll_loss(output, target) + US\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_ = None\n",
    "target_ = None\n",
    "def train(epoch):\n",
    "    global data_, target_\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target) # Wrap them in Variable\n",
    "        optimizer.zero_grad() # Zero the parameter gradients\n",
    "        data = data.view(data.size()[0], 28*28)\n",
    "        #print(data.data.min(), data.data.max(), data[0].data.min(), data[0].data.max())\n",
    "        #data2 = data.clone()\n",
    "        '''\n",
    "        for flips in range(3):\n",
    "            if flips == 0:\n",
    "                hshift = np.random.randint(10)\n",
    "                vshift = np.random.randint(10)\n",
    "                data2 = copy.deepcopy(data.resize_as(torch.FloatTensor(data.size()[0], 28, 28)))\n",
    "                #data2 = data.resize_as(torch.FloatTensor(data.size()[0], 28, 28))\n",
    "                if 1:\n",
    "                    data2[:,:,hshift:] = data2[:,:,:28-hshift]\n",
    "                    data2[:,:,:hshift] = 0\n",
    "                    #data2 = torch.cat((torch.FloatTensor(data.size()[0], 28,hshift), data2[:,:,hshift:]))\n",
    "            if flips == 1:\n",
    "                data2 = data.resize_as(torch.FloatTensor(data.size()[0], 28, 28))\n",
    "                data2 = data2.transpose(1,2)\n",
    "                data2 = data2.resize_as(torch.FloatTensor(data.size()[0], 784))\n",
    "            if flips == 2:\n",
    "                data2 = data\n",
    "        '''\n",
    "        outputs = model(data,noise=2e-1) # Forward \n",
    "        #print('yhat size',outputs[0].size())\n",
    "        output = outputs[0]\n",
    "        US = outputs[1]\n",
    "        loss = F.nll_loss(output, target) + US*.1\n",
    "        #print('loss', loss)\n",
    "        #print('yhat', output[0])\n",
    "        #print('unsupervised loss', US)\n",
    "        #print('weight reg', outputs[2])\n",
    "        #print('encoder error', outputs[3])\n",
    "        #best_optim = copy.copy(optimizer)\n",
    "        loss.backward()\n",
    "        norm = 0\n",
    "        for p in model.parameters():\n",
    "            norm += (p.grad.data ** 2).sum()\n",
    "        norm = np.sqrt(norm)\n",
    "        #if norm > 1:\n",
    "        #    for p in model.parameters():\n",
    "        #        p.grad.data /= norm\n",
    "        for p in model.parameters():\n",
    "            if anynan(p):\n",
    "                print(p)\n",
    "                print(nan(p), inf(p), big(p))\n",
    "            assert not anynan(p)\n",
    "            if anynan(p.grad):\n",
    "                print(p.grad)\n",
    "                print(nan(p.grad), inf(p.grad), big(p.grad))\n",
    "                data_ = data\n",
    "                target_ = target\n",
    "            assert not anynan(p.grad)\n",
    "        optimizer.step()\n",
    "        for p in model.parameters():\n",
    "            if anynan(p):\n",
    "                print(p)\n",
    "                print(nan(p), inf(p), big(p))\n",
    "            assert not anynan(p)\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__doc__',\n",
       " '__format__',\n",
       " '__getattribute__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__iter__',\n",
       " '__len__',\n",
       " '__module__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'batch_size',\n",
       " 'collate_fn',\n",
       " 'dataset',\n",
       " 'num_workers',\n",
       " 'pin_memory',\n",
       " 'sampler']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.843482239474\n"
     ]
    }
   ],
   "source": [
    "lshift = np.random.random()\n",
    "print(lshift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test(epoch, valid_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in valid_loader:\n",
    "            data, target = Variable(data, volatile=True), Variable(target)\n",
    "            data = data.view(data.size()[0], 28*28)\n",
    "            outputs = model(data, noise = 0)\n",
    "            output = outputs[0]\n",
    "            test_loss += F.nll_loss(output, target).data[0]\n",
    "            pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    test_loss /= len(valid_loader) # loss function already averages over batch size\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(valid_loader.dataset),\n",
    "        100. * correct / len(valid_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.0\n",
      "Train Epoch: 1 [0/78000 (0%)]\tLoss: 0.648917\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "Train Epoch: 1 [640/78000 (1%)]\tLoss: 0.673932\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "0.996078431606 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "Train Epoch: 1 [1280/78000 (2%)]\tLoss: 0.751430\n",
      "0.996078431606 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-12:\n",
      "Process Process-11:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib64/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gq/.local/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 26, in _worker_loop\n",
      "  File \"/usr/lib64/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    r = index_queue.get()\n",
      "    self.run()\n",
      "  File \"/usr/lib64/python2.7/multiprocessing/queues.py\", line 378, in get\n",
      "  File \"/usr/lib64/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    return recv()\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/gq/.local/lib/python2.7/site-packages/torch/multiprocessing/queue.py\", line 21, in recv\n",
      "  File \"/home/gq/.local/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 26, in _worker_loop\n",
      "    buf = self.recv_bytes()\n",
      "    r = index_queue.get()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib64/python2.7/multiprocessing/queues.py\", line 376, in get\n",
      "    racquire()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-41-652d2c59e591>\", line 5, in <module>\n",
      "    train(epoch)\n",
      "  File \"<ipython-input-37-96893cf3a8d8>\", line 62, in train\n",
      "    optimizer.step()\n",
      "  File \"/home/gq/.local/lib/python2.7/site-packages/torch/optim/adam.py\", line 64, in step\n",
      "    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.0\n",
      "> /home/gq/.local/lib/python2.7/site-packages/torch/optim/adam.py(64)step()\n",
      "-> exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-652d2c59e591>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_exc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_mortem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib64/python2.7/pdb.pyc\u001b[0m in \u001b[0;36mpost_mortem\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPdb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python2.7/pdb.pyc\u001b[0m in \u001b[0;36minteraction\u001b[0;34m(self, frame, traceback)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_stack_entry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmdloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python2.7/cmd.pyc\u001b[0m in \u001b[0;36mcmdloop\u001b[0;34m(self, intro)\u001b[0m\n\u001b[1;32m    128\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_rawinput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m                         \u001b[0;32mexcept\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'EOF'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gq/.local/lib/python2.7/site-packages/ipykernel/kernelbase.pyc\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gq/.local/lib/python2.7/site-packages/ipykernel/kernelbase.pyc\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    722\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pdb, traceback, sys\n",
    "\n",
    "try:\n",
    "    for epoch in range(1, 30):\n",
    "        train(epoch)\n",
    "        test(epoch, valid_loader)\n",
    "except:\n",
    "    t, v, tb = sys.exc_info()\n",
    "    traceback.print_exc()\n",
    "    pdb.post_mortem(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'RandomHorizontalFlip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-760-67b2031bb4e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomHorizontalFlip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'RandomHorizontalFlip'"
     ]
    }
   ],
   "source": [
    "a.RandomHorizontalFlip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'arr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-480-c10a5464e1f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'arr' is not defined"
     ]
    }
   ],
   "source": [
    "torch.from_numpy(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arr = np.ones(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-482-07016687bdda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masNumpyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "a.asNumpyarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-483-60b725f10c9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = list(range(784,-1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dataarr[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'image'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-486-5350a1bf76ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'image'"
     ]
    }
   ],
   "source": [
    "torch.image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataarr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-487-f2af4eacecd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataarr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dataarr' is not defined"
     ]
    }
   ],
   "source": [
    "dataarr.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'torchvision.transforms' from '/home/lee/anaconda3/lib/python3.5/site-packages/torchvision-0.1.6-py3.5.egg/torchvision/transforms.py'>"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n",
      "Train Epoch: 1 [0/78000 (0%)]\tLoss: 17.828180\n",
      "torch.Size([64, 10])\n",
      "torch.Size([64, 10])\n",
      "torch.Size([64, 10])\n",
      "torch.Size([64, 10])\n",
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    data, target = Variable(data), Variable(target) # Wrap them in Variable\n",
    "    optimizer.zero_grad() # Zero the parameter gradients\n",
    "    data = data.view(data.size()[0], 28*28)\n",
    "    #data2 = data.clone()\n",
    "    '''\n",
    "    for flips in range(3):\n",
    "        if flips == 0:\n",
    "            hshift = np.random.randint(10)\n",
    "            vshift = np.random.randint(10)\n",
    "            data2 = copy.deepcopy(data.resize_as(torch.FloatTensor(data.size()[0], 28, 28)))\n",
    "            #data2 = data.resize_as(torch.FloatTensor(data.size()[0], 28, 28))\n",
    "            if 1:\n",
    "                data2[:,:,hshift:] = data2[:,:,:28-hshift]\n",
    "                data2[:,:,:hshift] = 0\n",
    "                #data2 = torch.cat((torch.FloatTensor(data.size()[0], 28,hshift), data2[:,:,hshift:]))\n",
    "        if flips == 1:\n",
    "            data2 = data.resize_as(torch.FloatTensor(data.size()[0], 28, 28))\n",
    "            data2 = data2.transpose(1,2)\n",
    "            data2 = data2.resize_as(torch.FloatTensor(data.size()[0], 784))\n",
    "        if flips == 2:\n",
    "            data2 = data\n",
    "    '''\n",
    "    outputs = model(data,noise=2e-1) # Forward \n",
    "    #print(outputs[0].size())\n",
    "    output = outputs[0]\n",
    "    US = outputs[1]\n",
    "    loss = F.nll_loss(output, target) + US*.1\n",
    "    #print('loss', loss)\n",
    "    #print('yhat', output)\n",
    "    #print('unsupervised loss', US)\n",
    "    #print('weight reg', outputs[2])\n",
    "    #print('encoder error', outputs[3])\n",
    "    if np.isnan(outputs[2].data.numpy()) or np.isnan(outputs[3].data.numpy()):\n",
    "        break\n",
    "    loss.backward() \n",
    "    optimizer.step()\n",
    "    if batch_idx % 10 == 0:\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "clamp_",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-620-b59a53159bf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'param_groups'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;31m#return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lee/anaconda3/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fallthrough_methods\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: clamp_"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    data, target = Variable(data), Variable(target) # Wrap them in Variable\n",
    "    optimizer.zero_grad() # Zero the parameter gradients\n",
    "    data = data.view(data.size()[0], 28*28)\n",
    "    #data2 = data.clone()\n",
    "    '''\n",
    "    for flips in range(3):\n",
    "        if flips == 0:\n",
    "            hshift = np.random.randint(10)\n",
    "            vshift = np.random.randint(10)\n",
    "            data2 = copy.deepcopy(data.resize_as(torch.FloatTensor(data.size()[0], 28, 28)))\n",
    "            #data2 = data.resize_as(torch.FloatTensor(data.size()[0], 28, 28))\n",
    "            if 1:\n",
    "                data2[:,:,hshift:] = data2[:,:,:28-hshift]\n",
    "                data2[:,:,:hshift] = 0\n",
    "                #data2 = torch.cat((torch.FloatTensor(data.size()[0], 28,hshift), data2[:,:,hshift:]))\n",
    "        if flips == 1:\n",
    "            data2 = data.resize_as(torch.FloatTensor(data.size()[0], 28, 28))\n",
    "            data2 = data2.transpose(1,2)\n",
    "            data2 = data2.resize_as(torch.FloatTensor(data.size()[0], 784))\n",
    "        if flips == 2:\n",
    "            data2 = data\n",
    "    '''\n",
    "    outputs = model(data,noise=2e-1) # Forward \n",
    "    #print('yhat size',outputs[0].size())\n",
    "    output = outputs[0]\n",
    "    US = outputs[1]\n",
    "    loss = F.nll_loss(output, target) + US*.1\n",
    "    #print('loss', loss)\n",
    "    #print('yhat', output[0])\n",
    "    #print('unsupervised loss', US)\n",
    "    #print('weight reg', outputs[2])\n",
    "    #print('encoder error', outputs[3])\n",
    "    loss.backward()\n",
    "    for p in optimizer.state_dict()['param_groups'][0]['params']:\n",
    "        p.grad.data.clamp_(-1, 1)\n",
    "    #return\n",
    "    optimizer.step()\n",
    "    if batch_idx % 10 == 0:\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.autograd.variable.Variable"
      ]
     },
     "execution_count": 622,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
