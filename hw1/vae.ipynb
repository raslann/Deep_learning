{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded\n",
      "3000\n",
      "750\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import pickle \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from sub import subMNIST\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Set labels to zeros as a dummy values to make the train_loader work \n",
    "\n",
    "trainset_unlabeled = pickle.load(open(\"train_unlabeled.p\", \"rb\")) \n",
    "trainset_unlabeled.train_labels = torch.zeros(trainset_unlabeled.train_data.size(0))\n",
    "train_loader  = torch.utils.data.DataLoader(trainset_unlabeled, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "validset = pickle.load(open(\"validation.p\", \"rb\"))\n",
    "valid_loader = torch.utils.data.DataLoader(validset, batch_size=64, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE (\n",
      "  (fc1): Linear (784 -> 400)\n",
      "  (fc21): Linear (400 -> 20)\n",
      "  (fc22): Linear (400 -> 20)\n",
      "  (fc3): Linear (20 -> 400)\n",
      "  (fc4): Linear (400 -> 784)\n",
      "  (relu): ReLU ()\n",
      "  (sigmoid): Sigmoid ()\n",
      ")\n",
      "Models has 10 learnable paramater:\n",
      "parameter 1 has a size of torch.Size([400, 784])\n",
      "parameter 2 has a size of torch.Size([400])\n",
      "parameter 3 has a size of torch.Size([20, 400])\n",
      "parameter 4 has a size of torch.Size([20])\n",
      "parameter 5 has a size of torch.Size([20, 400])\n",
      "parameter 6 has a size of torch.Size([20])\n",
      "parameter 7 has a size of torch.Size([400, 20])\n",
      "parameter 8 has a size of torch.Size([400])\n",
      "parameter 9 has a size of torch.Size([784, 400])\n",
      "parameter 10 has a size of torch.Size([784])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    # q(z|x) -> Bayesian Perspective\n",
    "    def encode(self, x):\n",
    "        h1 = self.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    # reparameterization trick to make the sample subject to random noise \n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    # p(x|z) -> Generative Perspective\n",
    "    def decode(self, z):\n",
    "        h3 = self.relu(self.fc3(z))\n",
    "        return self.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "\n",
    "model = VAE()\n",
    "params = list(model.parameters())\n",
    "\n",
    "print(model)\n",
    "print('Models has {} learnable paramater:'.format(len(params)))\n",
    "[print('parameter {} has a size of {}'.format(i+1, params[i].size())) for i in range(len(params))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reconstruction_function = nn.BCELoss()\n",
    "reconstruction_function.size_average = False\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar): \n",
    "    \"\"\"(Gaussian) Kullback-Leibler divergence KL(q||p), per training example\"\"\"\n",
    "    \n",
    "    BCE = reconstruction_function(recon_x, x)\n",
    "    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
    "    KLD = torch.sum(KLD_element).mul_(-0.5)\n",
    "    return BCE + KLD\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1109c2668>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10fec0080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Test Image\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "test_image = images[1][0]\n",
    "np_test_image = test_image.numpy()\n",
    "plt.imshow(np_test_image, cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x110bdd5f8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1156ea080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Ask the model before traning to reconstrcut the \"test image\"\n",
    "\n",
    "output = model(Variable(test_image))\n",
    "reconstrcuted_image = output[0].data.view(28,28).numpy()\n",
    "plt.imshow(reconstrcuted_image, cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = Variable(data)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.data[0]\n",
    "        optimizer.step()\n",
    "        if batch_idx % 20 == 0:\n",
    "            \n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.data[0] / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))\n",
    "    \n",
    "    # Ask the model after training to reconstrcut the \"test image\"\n",
    "    \n",
    "    output = model(Variable(test_image)) \n",
    "    reconstrcuted_image = output[0].data.view(28,28).numpy()\n",
    "    plt.imshow(reconstrcuted_image, cmap = 'gray')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    for data, _ in valid_loader:\n",
    "        data = Variable(data, volatile=True)\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        test_loss += loss_function(recon_batch, data, mu, logvar).data[0] \n",
    "\n",
    "    test_loss /= len(valid_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/47000 (0%)]\tLoss: 550.859436\n",
      "Train Epoch: 1 [1280/47000 (3%)]\tLoss: -3944.977051\n",
      "Train Epoch: 1 [2560/47000 (5%)]\tLoss: -4929.130859\n",
      "Train Epoch: 1 [3840/47000 (8%)]\tLoss: -5064.358887\n",
      "Train Epoch: 1 [5120/47000 (11%)]\tLoss: -5892.219238\n",
      "Train Epoch: 1 [6400/47000 (14%)]\tLoss: -6241.758301\n",
      "Train Epoch: 1 [7680/47000 (16%)]\tLoss: -6994.364746\n",
      "Train Epoch: 1 [8960/47000 (19%)]\tLoss: -7292.389160\n",
      "Train Epoch: 1 [10240/47000 (22%)]\tLoss: -7662.237793\n",
      "Train Epoch: 1 [11520/47000 (24%)]\tLoss: -7747.131348\n",
      "Train Epoch: 1 [12800/47000 (27%)]\tLoss: -8094.465820\n",
      "Train Epoch: 1 [14080/47000 (30%)]\tLoss: -8057.169922\n",
      "Train Epoch: 1 [15360/47000 (33%)]\tLoss: -7998.875000\n",
      "Train Epoch: 1 [16640/47000 (35%)]\tLoss: -8354.772461\n",
      "Train Epoch: 1 [17920/47000 (38%)]\tLoss: -8640.177734\n",
      "Train Epoch: 1 [19200/47000 (41%)]\tLoss: -8696.521484\n",
      "Train Epoch: 1 [20480/47000 (44%)]\tLoss: -8841.162109\n",
      "Train Epoch: 1 [21760/47000 (46%)]\tLoss: -8649.681641\n",
      "Train Epoch: 1 [23040/47000 (49%)]\tLoss: -8691.823242\n",
      "Train Epoch: 1 [24320/47000 (52%)]\tLoss: -8653.281250\n",
      "Train Epoch: 1 [25600/47000 (54%)]\tLoss: -9188.573242\n",
      "Train Epoch: 1 [26880/47000 (57%)]\tLoss: -9033.902344\n",
      "Train Epoch: 1 [28160/47000 (60%)]\tLoss: -8995.048828\n",
      "Train Epoch: 1 [29440/47000 (63%)]\tLoss: -9011.883789\n",
      "Train Epoch: 1 [30720/47000 (65%)]\tLoss: -9189.116211\n",
      "Train Epoch: 1 [32000/47000 (68%)]\tLoss: -9140.560547\n",
      "Train Epoch: 1 [33280/47000 (71%)]\tLoss: -9073.270508\n",
      "Train Epoch: 1 [34560/47000 (73%)]\tLoss: -8859.284180\n",
      "Train Epoch: 1 [35840/47000 (76%)]\tLoss: -9236.833984\n",
      "Train Epoch: 1 [37120/47000 (79%)]\tLoss: -9259.318359\n",
      "Train Epoch: 1 [38400/47000 (82%)]\tLoss: -9019.118164\n",
      "Train Epoch: 1 [39680/47000 (84%)]\tLoss: -9530.403320\n",
      "Train Epoch: 1 [40960/47000 (87%)]\tLoss: -8792.708984\n",
      "Train Epoch: 1 [42240/47000 (90%)]\tLoss: -9330.953125\n",
      "Train Epoch: 1 [43520/47000 (93%)]\tLoss: -9549.520508\n",
      "Train Epoch: 1 [44800/47000 (95%)]\tLoss: -9253.333984\n",
      "Train Epoch: 1 [46080/47000 (98%)]\tLoss: -9248.830078\n",
      "====> Epoch: 1 Average loss: -8102.7984\n",
      "====> Test set loss: -9560.7604\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8ANJEX4gKcIPJEX4gaQIP5AU4QeS+h8ZlhqGlV4e1wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111055eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(1, 2):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The start of the semi_supervised model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainset_labeled = pickle.load(open(\"train_labeled.p\", \"rb\")) \n",
    "train_loader  = torch.utils.data.DataLoader(trainset_labeled, batch_size=64, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for p in model.parameters():\n",
    "     p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SemiSupervised(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SemiSupervised, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(20, 15)\n",
    "        self.fc2 = nn.Linear(15, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        mu, logvar = model.encode(x.view(-1, 784))\n",
    "        z = model.reparametrize(mu, logvar)\n",
    "        x = F.relu(self.fc1(z))\n",
    "        return F.sigmoid(F.relu(self.fc2(x)))\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "semi_model = SemiSupervised()\n",
    "optimizer = optim.Adam(semi_model.parameters(), lr=1e-3)\n",
    "loss_function = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        data = Variable(data)\n",
    "        target = Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = semi_model(data) \n",
    "        loss =  loss_function(output, target) \n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(epoch, valid_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in valid_loader:\n",
    "\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = semi_model(data)\n",
    "        test_loss += loss_function(output, target).data[0]\n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    test_loss /= len(valid_loader) # loss function already averages over batch size\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(valid_loader.dataset),\n",
    "        100. * correct / len(valid_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/3000 (0%)]\tLoss: 2.276165\n",
      "Train Epoch: 1 [640/3000 (21%)]\tLoss: 2.277667\n",
      "Train Epoch: 1 [1280/3000 (43%)]\tLoss: 2.244603\n",
      "Train Epoch: 1 [1920/3000 (64%)]\tLoss: 2.240822\n",
      "Train Epoch: 1 [2560/3000 (85%)]\tLoss: 2.221900\n",
      "\n",
      "Test set: Average loss: 2.2089, Accuracy: 3640/10000 (36%)\n",
      "\n",
      "Train Epoch: 2 [0/3000 (0%)]\tLoss: 2.191455\n",
      "Train Epoch: 2 [640/3000 (21%)]\tLoss: 2.182140\n",
      "Train Epoch: 2 [1280/3000 (43%)]\tLoss: 2.165468\n",
      "Train Epoch: 2 [1920/3000 (64%)]\tLoss: 2.182326\n",
      "Train Epoch: 2 [2560/3000 (85%)]\tLoss: 2.102584\n",
      "\n",
      "Test set: Average loss: 2.1349, Accuracy: 5458/10000 (55%)\n",
      "\n",
      "Train Epoch: 3 [0/3000 (0%)]\tLoss: 2.115494\n",
      "Train Epoch: 3 [640/3000 (21%)]\tLoss: 2.118392\n",
      "Train Epoch: 3 [1280/3000 (43%)]\tLoss: 2.106713\n",
      "Train Epoch: 3 [1920/3000 (64%)]\tLoss: 2.113840\n",
      "Train Epoch: 3 [2560/3000 (85%)]\tLoss: 2.122356\n",
      "\n",
      "Test set: Average loss: 2.0780, Accuracy: 6532/10000 (65%)\n",
      "\n",
      "Train Epoch: 4 [0/3000 (0%)]\tLoss: 2.062775\n",
      "Train Epoch: 4 [640/3000 (21%)]\tLoss: 2.045774\n",
      "Train Epoch: 4 [1280/3000 (43%)]\tLoss: 2.064145\n",
      "Train Epoch: 4 [1920/3000 (64%)]\tLoss: 2.050590\n",
      "Train Epoch: 4 [2560/3000 (85%)]\tLoss: 2.028256\n",
      "\n",
      "Test set: Average loss: 2.0430, Accuracy: 7063/10000 (71%)\n",
      "\n",
      "Train Epoch: 5 [0/3000 (0%)]\tLoss: 2.032298\n",
      "Train Epoch: 5 [640/3000 (21%)]\tLoss: 1.997715\n",
      "Train Epoch: 5 [1280/3000 (43%)]\tLoss: 2.026886\n",
      "Train Epoch: 5 [1920/3000 (64%)]\tLoss: 2.000128\n",
      "Train Epoch: 5 [2560/3000 (85%)]\tLoss: 2.029286\n",
      "\n",
      "Test set: Average loss: 2.0189, Accuracy: 7401/10000 (74%)\n",
      "\n",
      "Train Epoch: 6 [0/3000 (0%)]\tLoss: 2.002436\n",
      "Train Epoch: 6 [640/3000 (21%)]\tLoss: 2.043744\n",
      "Train Epoch: 6 [1280/3000 (43%)]\tLoss: 2.019272\n",
      "Train Epoch: 6 [1920/3000 (64%)]\tLoss: 1.995625\n",
      "Train Epoch: 6 [2560/3000 (85%)]\tLoss: 1.988199\n",
      "\n",
      "Test set: Average loss: 2.0002, Accuracy: 7658/10000 (77%)\n",
      "\n",
      "Train Epoch: 7 [0/3000 (0%)]\tLoss: 1.971627\n",
      "Train Epoch: 7 [640/3000 (21%)]\tLoss: 1.974325\n",
      "Train Epoch: 7 [1280/3000 (43%)]\tLoss: 1.956813\n",
      "Train Epoch: 7 [1920/3000 (64%)]\tLoss: 1.990855\n",
      "Train Epoch: 7 [2560/3000 (85%)]\tLoss: 1.982607\n",
      "\n",
      "Test set: Average loss: 1.9875, Accuracy: 7828/10000 (78%)\n",
      "\n",
      "Train Epoch: 8 [0/3000 (0%)]\tLoss: 2.000039\n",
      "Train Epoch: 8 [640/3000 (21%)]\tLoss: 1.961200\n",
      "Train Epoch: 8 [1280/3000 (43%)]\tLoss: 1.976224\n",
      "Train Epoch: 8 [1920/3000 (64%)]\tLoss: 1.974601\n",
      "Train Epoch: 8 [2560/3000 (85%)]\tLoss: 2.009989\n",
      "\n",
      "Test set: Average loss: 1.9795, Accuracy: 7958/10000 (80%)\n",
      "\n",
      "Train Epoch: 9 [0/3000 (0%)]\tLoss: 1.961898\n",
      "Train Epoch: 9 [640/3000 (21%)]\tLoss: 1.980259\n",
      "Train Epoch: 9 [1280/3000 (43%)]\tLoss: 1.958768\n",
      "Train Epoch: 9 [1920/3000 (64%)]\tLoss: 1.947696\n",
      "Train Epoch: 9 [2560/3000 (85%)]\tLoss: 1.944906\n",
      "\n",
      "Test set: Average loss: 1.9733, Accuracy: 8059/10000 (81%)\n",
      "\n",
      "Train Epoch: 10 [0/3000 (0%)]\tLoss: 1.992164\n",
      "Train Epoch: 10 [640/3000 (21%)]\tLoss: 1.957621\n",
      "Train Epoch: 10 [1280/3000 (43%)]\tLoss: 1.959331\n",
      "Train Epoch: 10 [1920/3000 (64%)]\tLoss: 1.980273\n",
      "Train Epoch: 10 [2560/3000 (85%)]\tLoss: 1.972715\n",
      "\n",
      "Test set: Average loss: 1.9687, Accuracy: 8086/10000 (81%)\n",
      "\n",
      "Train Epoch: 11 [0/3000 (0%)]\tLoss: 1.962528\n",
      "Train Epoch: 11 [640/3000 (21%)]\tLoss: 1.956538\n",
      "Train Epoch: 11 [1280/3000 (43%)]\tLoss: 1.985222\n",
      "Train Epoch: 11 [1920/3000 (64%)]\tLoss: 1.987276\n",
      "Train Epoch: 11 [2560/3000 (85%)]\tLoss: 1.923379\n",
      "\n",
      "Test set: Average loss: 1.9647, Accuracy: 8137/10000 (81%)\n",
      "\n",
      "Train Epoch: 12 [0/3000 (0%)]\tLoss: 1.947932\n",
      "Train Epoch: 12 [640/3000 (21%)]\tLoss: 1.967495\n",
      "Train Epoch: 12 [1280/3000 (43%)]\tLoss: 1.953436\n",
      "Train Epoch: 12 [1920/3000 (64%)]\tLoss: 1.959425\n",
      "Train Epoch: 12 [2560/3000 (85%)]\tLoss: 1.946830\n",
      "\n",
      "Test set: Average loss: 1.9617, Accuracy: 8185/10000 (82%)\n",
      "\n",
      "Train Epoch: 13 [0/3000 (0%)]\tLoss: 1.931133\n",
      "Train Epoch: 13 [640/3000 (21%)]\tLoss: 1.976685\n",
      "Train Epoch: 13 [1280/3000 (43%)]\tLoss: 1.943988\n",
      "Train Epoch: 13 [1920/3000 (64%)]\tLoss: 1.937417\n",
      "Train Epoch: 13 [2560/3000 (85%)]\tLoss: 1.956124\n",
      "\n",
      "Test set: Average loss: 1.9591, Accuracy: 8162/10000 (82%)\n",
      "\n",
      "Train Epoch: 14 [0/3000 (0%)]\tLoss: 1.949978\n",
      "Train Epoch: 14 [640/3000 (21%)]\tLoss: 1.959773\n",
      "Train Epoch: 14 [1280/3000 (43%)]\tLoss: 1.952478\n",
      "Train Epoch: 14 [1920/3000 (64%)]\tLoss: 1.918345\n",
      "Train Epoch: 14 [2560/3000 (85%)]\tLoss: 1.960525\n",
      "\n",
      "Test set: Average loss: 1.9565, Accuracy: 8223/10000 (82%)\n",
      "\n",
      "Train Epoch: 15 [0/3000 (0%)]\tLoss: 1.932348\n",
      "Train Epoch: 15 [640/3000 (21%)]\tLoss: 1.938762\n",
      "Train Epoch: 15 [1280/3000 (43%)]\tLoss: 1.950839\n",
      "Train Epoch: 15 [1920/3000 (64%)]\tLoss: 1.964407\n",
      "Train Epoch: 15 [2560/3000 (85%)]\tLoss: 1.916453\n",
      "\n",
      "Test set: Average loss: 1.9549, Accuracy: 8238/10000 (82%)\n",
      "\n",
      "Train Epoch: 16 [0/3000 (0%)]\tLoss: 1.949013\n",
      "Train Epoch: 16 [640/3000 (21%)]\tLoss: 1.948815\n",
      "Train Epoch: 16 [1280/3000 (43%)]\tLoss: 1.928083\n",
      "Train Epoch: 16 [1920/3000 (64%)]\tLoss: 1.934760\n",
      "Train Epoch: 16 [2560/3000 (85%)]\tLoss: 1.931356\n",
      "\n",
      "Test set: Average loss: 1.9528, Accuracy: 8240/10000 (82%)\n",
      "\n",
      "Train Epoch: 17 [0/3000 (0%)]\tLoss: 1.951289\n",
      "Train Epoch: 17 [640/3000 (21%)]\tLoss: 1.941229\n",
      "Train Epoch: 17 [1280/3000 (43%)]\tLoss: 1.942083\n",
      "Train Epoch: 17 [1920/3000 (64%)]\tLoss: 1.940412\n",
      "Train Epoch: 17 [2560/3000 (85%)]\tLoss: 1.935596\n",
      "\n",
      "Test set: Average loss: 1.9516, Accuracy: 8244/10000 (82%)\n",
      "\n",
      "Train Epoch: 18 [0/3000 (0%)]\tLoss: 1.942435\n",
      "Train Epoch: 18 [640/3000 (21%)]\tLoss: 1.956148\n",
      "Train Epoch: 18 [1280/3000 (43%)]\tLoss: 1.955184\n",
      "Train Epoch: 18 [1920/3000 (64%)]\tLoss: 1.944104\n",
      "Train Epoch: 18 [2560/3000 (85%)]\tLoss: 1.927996\n",
      "\n",
      "Test set: Average loss: 1.9501, Accuracy: 8288/10000 (83%)\n",
      "\n",
      "Train Epoch: 19 [0/3000 (0%)]\tLoss: 1.948945\n",
      "Train Epoch: 19 [640/3000 (21%)]\tLoss: 1.934486\n",
      "Train Epoch: 19 [1280/3000 (43%)]\tLoss: 1.944947\n",
      "Train Epoch: 19 [1920/3000 (64%)]\tLoss: 1.959934\n",
      "Train Epoch: 19 [2560/3000 (85%)]\tLoss: 1.933561\n",
      "\n",
      "Test set: Average loss: 1.9491, Accuracy: 8272/10000 (83%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 20):\n",
    "    train(epoch)\n",
    "    test(epoch, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
